#+TITLE: Work
#+STARTUP: latexpreview overview hideblocks
#+OPTIONS: toc:nil num:nil email:t ^:nil

* Personal Projects
** CV
write an org exporter for my CV using awesome-cv.
ox-awesomecv

https://github.com/posquit0/Awesome-CV
https://github.com/Titan-C/org-cv

** Start a blog
- It will not be specialized

** Calendar musings
- What's the probability of August starting on a Monday?
[[https://en.wikipedia.org/wiki/Zeller%2527s_congruence][Zeller's congruence]]

** TODO dotfiles

- State "TODO"       from              [2019-07-22 Mon 13:35]

Tasks:

- [ ] virtual machine (personal laptop or work laptop?)
- [ ] change emacs repo to dotfiles and stow
- [ ] firefox dotfiles?
- [ ] add termux dotfiles

Roadmap:
- [ ] GNU Guix (GNOME)
- [ ] Emacs config
- [ ] Firefox
- [ ] Nextcloud
- [ ] Telegram
- [ ] Paromtv
- [ ] Termux dotfiles
- [ ] packages
   + git
   + stow
   + emacs
   + tor
   + telegram
   + docker

** visit linkedin without account
[[https://github.com/tothi/linkedin-auth-bypass][solution]]
[[https://search.google.com/test/mobile-friendly][mobile friendly]]

** hack mubi.com
retrieve my favourite movies

* Bold
:PROPERTIES:
:START: [2019-09-16]
:FINISH:
:END:
Eusoubold1

[[pdfview:/media/sf_vm_bridge/gomezand-grid.pdf::1][Adidas grid]]

|   | A | B | C | D | E | F | G | H | I | J |
|---+---+---+---+---+---+---+---+---+---+---|
| 1 | Y | 6 | C | 4 | W | X | X | 5 | K | 5 |
| 2 | K | J | E | 1 | D | 2 | J | R | 0 | F |
| 3 | 3 | F | 8 | M | 0 | 4 | 2 | 7 | V | 6 |
| 4 | 6 | 5 | E | Q | F | E | 0 | 2 | 3 | 1 |
| 5 | 9 | 8 | 6 | 9 | 3 | 9 | 0 | 6 | P | 9 |

** Tutorials
- [[https://docs.docker.com/get-started/][Docker]]
- [[https://docs.docker.com/get-started/][Airflow]]
- [[https://cs.stanford.edu/people/widom/DB-mooc.html][database course]]
- [[https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7][Data Engineering basics]]

** Bureaucracy
*** Address
Bold International
Avenida D. Joao II, nr 43, 9
Lisbon 1990-084
Portugal

NIF
509216595

*** TODO Timesheet
DEADLINE: <2019-10-18 Fri +1m -2d>

[[https://portal.boldint.com/timesheet/timesheet][link]]

Eusoubold1

*** TODO Medical Exams
<2019-09-26 Thu 16:15>

<2019-09-26 Thu 15:30>

Oriente -> S. Sebastiao -> Marques

leave laptop at the office.

[[file:~/NextCloud/tmp/medicina_trabalho.pdf][+info]]

*** Plan Holidays
how many days do I have?

** Meetings
<2019-09-26 Thu 9:30-12:00>
Onboarding

* Jobs
[[https://harryrschwartz.com/assets/documents/articles/finding-jobs-in-software.pdf][Harry Schwartz - Finding Jobs in Software]]

** Companies
- TravisCI
- Yarilabs (Haskell)
- Checkmarx (Haskell)
- Subvisual (Haskell)
- Wikipedia
- O'Reilly
- Lifeonmars / Fractal
- FPComplete
- Toptal [2019-08-19]
- DefinedCrowd [2019-08-08]
- Freiheit [2019-08-06]
- [[https://scopicsoftware.com/careers/#op-335924-remote-slam-algorithm-developer][Scopic Software]] [2019-07-18]
- Siscog [2019-06-11]
- [[http://mediterra-soft.com/][M-soft]] [2019-08-29]

*** Smart Steel Technologies                                :ARCHIVE:
:PROPERTIES:
:header-args: :tangle ./script.py
:END:
- State "DONE"       from "TODO"       [2019-07-20 Sat 00:00]
- State "TODO"       from              [2019-07-13 Sat 23:58]

# (org-babel-tangle-file "work.org")

**** Test
***** Analysis

Let's import the data and get acquainted with it.

#+begin_src ipython :results output :session
  import pandas as pd

  data = pd.read_csv("~/Downloads/task/task/task_data.csv",
                     index_col="sample index")

  print(data.drop(columns="class_label").describe())
#+end_src

#+RESULTS:
#+begin_example
          sensor0     sensor1     sensor2     sensor3     sensor4     sensor5  \
count  400.000000  400.000000  400.000000  400.000000  400.000000  400.000000
mean     0.523661    0.509223    0.481238    0.509752    0.497875    0.501065
std      0.268194    0.276878    0.287584    0.297712    0.288208    0.287634
min      0.007775    0.003865    0.004473    0.001466    0.000250    0.000425
25%      0.299792    0.283004    0.235544    0.262697    0.249369    0.269430
50%      0.534906    0.507583    0.460241    0.510066    0.497842    0.497108
75%      0.751887    0.727843    0.734937    0.768975    0.743401    0.738854
max      0.999476    0.998680    0.992963    0.995119    0.999412    0.997367

          sensor6     sensor7     sensor8     sensor9
count  400.000000  400.000000  400.000000  400.000000
mean     0.490480    0.482372    0.482822    0.541933
std      0.289954    0.282714    0.296180    0.272490
min      0.000173    0.003322    0.003165    0.000452
25%      0.226687    0.242848    0.213626    0.321264
50%      0.477341    0.463438    0.462251    0.578389
75%      0.735304    0.732483    0.740542    0.768990
max      0.997141    0.998230    0.996098    0.999465
#+end_example

First observation: all of the sensor data $\in [0,1]$. Since all of these
parameters are on the same scale, they're comparable.

Let's solve this problem in a very naive and simple way:

- Group the data by classes (1 and -1 in our case);
- Compute the mean (for each sensor, for each class);

This will give us an order in the sensors. The more relevant ones will be those
such that the mean relative to classes 1 and -1 differ by a greater amount.

Find an implementation below.

Firstly, I'm rescaling the data to follow a normal distribution of 0 mean and
standard deviation 1, \( \sim \mathcal{N}(0,1)\). Actually that is
unnecessary. I did it for the fun of it.

#+begin_src ipython :results output :session
  def order_sensors(data, preprocess=True):
      """Applies a preprocessing so that the data follows a normal distribution with
      mean 0 and standard deviation of 1.

      Then groups by the binary classification and ranks the sensors by how much
      each mean value differs in each binary class.

      :param data: pandas DataFrame
      :param preprocess: bool
      :returns: list of ordered sensors
      :rtype: list

      """
      if preprocess:
          from sklearn.preprocessing import StandardScaler

          data[data.columns.drop("class_label")] = StandardScaler().fit_transform(
              data[data.columns.drop("class_label")]
          )
      sensors_mean_per_class = data.groupby("class_label").mean()
      return (
          abs(sensors_mean_per_class.loc[-1] - sensors_mean_per_class.loc[1])
          .sort_values(ascending=False)
          .index.format()
      )


  print(*order_sensors(data), sep=" <\n< ")
#+end_src

#+RESULTS:
#+begin_example
sensor8 <
< sensor4 <
< sensor0 <
< sensor3 <
< sensor1 <
< sensor5 <
< sensor7 <
< sensor9 <
< sensor2 <
< sensor6
#+end_example

***** Strengths of this method
- The computations involved are basic

***** Weaknesses of this method
- Perhaps it is too simplistic/naive

***** How does it scale?
#+begin_src ipython :session :results output :async t
  def enlarge_dataset(data, factor_samples, factor_features):
      """Given a dataset of dimensions (n,m), it outputs the same data replicated in
      dimensions (factor_samples * n , factor_features * m).

      The class_label of binary classification is left untouched.

      :param data: pandas DataFrame with shape (n,m)
      :param factor_samples: int
      :param factor_features: int
      :returns: dataset with shape (factor_samples * n , factor_features * m)
      :rtype: pandas DataFrame

      """
      aux = pd.concat(
          [data.filter(regex="sensor*")] * factor_features, axis=1, ignore_index=True
      )
      aux["class_label"] = data["class_label"]
      return pd.concat([aux] * factor_samples, ignore_index=True)


  for samples in [1, 10, 100, 1000]:
      for sensors in [1, 10, 100]:
          %timeit order_sensors(enlarge_dataset(data, samples, sensors))
#+end_src

# #+RESULTS:
# : 6.21 ms ± 64.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
# : 8.44 ms ± 40.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
# : 21.5 ms ± 108 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
# : 7.33 ms ± 14.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
# : 16.3 ms ± 60.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
# : 111 ms ± 9.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
# : 17.7 ms ± 628 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
# : 102 ms ± 5.19 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
# : 2.01 s ± 178 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
# : 138 ms ± 7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
# : 1.79 s ± 66.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
# : Memory Error

| #samples | #sensors |   #values | time         |
|----------+----------+-----------+--------------|
|      400 |       10 |      4000 | 6.21 ms      |
|      400 |      100 |     40000 | 8.44 ms      |
|     4000 |       10 |     40000 | 7.33 ms      |
|      400 |     1000 |    400000 | 21.5 ms      |
|     4000 |      100 |    400000 | 16.3 ms      |
|    40000 |       10 |    400000 | 17.7 ms      |
|     4000 |     1000 |   4000000 | 111 ms       |
|    40000 |      100 |   4000000 | 102 ms       |
|   400000 |       10 |   4000000 | 138 ms       |
|    40000 |     1000 |  40000000 | 2.01 s       |
|   400000 |      100 |  40000000 | 1.79 s       |
|   400000 |     1000 | 400000000 | Memory Error |
#+TBLFM: $3=$1*$2

***** Alternative solutions
Perhaps another way would be to train a classification algorithm such as logistic
regression. The model would output coefficients for each parameter (sensor). The
decreasing order of these real coefficients would give us the order of the
sensors.

***** Versions
- Ubuntu 19.04
- GNU Emacs 26.1
- Org mode 9.2.4
- Python 3.7.3
- IPython 7.6.1
- Pandas 0.24.2
- Scikit-learn 0.21.2

***** About me
Find more about me at:
- [[https://github.com/aadcg][github]]
- [[https://cloud.owncube.com/s/KKDNy6nkr48cQWf][my cv]]
- [[https://math.stackexchange.com/users/380192/aadcg][stackexchange]]

***** About the document
This document was generated by Org mode - a major mode of GNU Emacs. What you
have seen above is literate programming.

Jupyter Notebooks and R Markdown are also implementations of literate
programming.

*** WAITING Унинум                                           :ARCHIVE:
- State "WAITING"    from              [2019-07-13 Sat 23:20] \\
  Did the test, Pavel liked.
[[https://habr.com/ru/post/457630/][article]]
[[file:~/NextCloud/train_dataset%20(1).csv][dataset]]

#+begin_src python :results output :session
  import pandas as pd
  import calendar

  dataset = pd.read_csv("../train_dataset (1).csv", index_col="receipt")


  dataset["pack1"] = 0
  dataset["pack2"] = 0
  dataset["pack3"] = 0
  dataset["pack1"].mask((dataset["racket"] == 1) & (dataset["shuttlecock"] == 2), 1, inplace=True
  )
  dataset["pack2"].mask(
      (dataset["racket"] == 1) & (dataset["shuttlecock"] == 5), 1, inplace=True
  )
  dataset["pack3"].mask(
      (dataset["racket"] == 0) & (dataset["shuttlecock"] == 10), 1, inplace=True
  )

  dataset = dataset.groupby("date").sum()
  dataset = dataset.reset_index()

  date = (
      dataset["date"]
      .str.split("-", expand=True)
      .drop(columns=[0])
      .rename(columns={1: "month", 2: "day_month"})
      .astype(int)
  )

  date["weekday"] = date.apply(
      lambda row: calendar.weekday(2018, row["month"], row["day_month"]), axis=1
  )

  dataset = pd.concat([dataset, date], axis=1)

  print(dataset.groupby('month').sum()['price'])
  print(max(dataset.groupby('month').sum()['price']))
  print(min(dataset.groupby('month').sum()['price']))

  # dataset = dataset.drop(columns=["month", "day_month"])
  # sum_packs =
  dataset["ratio"] = dataset["pack3"] / (
      (dataset["pack1"]) + (dataset["pack2"]) + (dataset["pack3"])
  )

  dataset_week = dataset.groupby("weekday").median()

  # | pack_name | # racket | # shuttlecocks | price(min) | price(max) | price(mean) |
  # |-----------+----------+----------------+------------+------------+-------------|
  # | pack1     |        1 |              2 |      14.88 |      15.36 |       15.12 |
  # | pack2     |        1 |              5 |      17.85 |      18.45 |       18.15 |
  # | pack3     |        0 |             10 |      12.90 |       13.3 |        13.1 |
  ,#+TBLFM: $5=vmean($3..$4)

  # 3 courts (max 12 people at the same time)

  # SALE JAN 2019
  # | product name | # racket | #shuttlecocks | price |
  # |--------------+----------+---------------+-------|
  # | pack1        |        1 |             0 | 11.80 |
  # | pack2        |        1 |             1 | 12.98 |
  # | pack3        |        0 |             5 |  5.90 |

  # dataset.drop(columns=['month', 'day_month']).groupby('weekday').mean()
  # dataset.drop(columns=['month', 'weekday']).groupby('day_month').sum()
  # dataset.drop(columns=['month', 'day_month', 'weekday']).groupby('date').sum()

  # dataset.groupby('date').sum().min()
  # racket           9.00
  # shuttlecock     66.00

  # dataset.groupby('date').sum().max()
  # racket           38.00
  # shuttlecock     225.00

  # dataset.groupby('date').sum().mean()
  # racket          21.495890
  # shuttlecock    150.224658


  def count_weekday(n):
      aux = dataset[["weekday", "date"]].drop_duplicates()
      return aux[aux["weekday"] == n].count()[0]


  # hist for rackets and shuttlecocks
  # dataset.groupby('date')['racket'].sum().hist()
  # dataset.groupby('date')['shuttlecock'].sum().hist()


  ts = pd.date_range(start="2019-01-01", end="2019-01-31", freq="D")
  predicted = pd.DataFrame(
      0, index=ts, columns=["pack1", "pack2", "pack3", "price"]
  )
  predicted = predicted.reset_index()
  predicted["index"] = predicted.astype("str")

  date = (
      predicted["index"]
      .str.split("-", expand=True)
      .drop(columns=[0])
      .rename(columns={1: "month", 2: "day_month"})
      .astype(int)
  )

  date["weekday"] = date.apply(
      lambda row: calendar.weekday(2019, row["month"], row["day_month"]), axis=1
  )

  predicted = pd.concat([predicted, date], axis=1)

  predicted = predicted.drop(columns=["month", "day_month"])

  dist = (
      dataset.groupby("weekday")
      .median()
      .round()[["pack1", "pack2", "pack3"]]
      .reset_index()
  )


  week_days = predicted.groupby('weekday').count()['index']

  dist['count'] = week_days

  dist["price"] = (
      (dist["pack1"] * 12.98 * dist["count"])
      + (dist["pack2"] * (5.90 + 11.80) * dist["count"])
      + (dist["pack3"] * 5.90 * dist["count"] * 2)
  )

  dist
#+end_src

#+begin_src python :results file
  import pandas as pd
  import calendar
  import matplotlib.pyplot as plt

  dataset = pd.read_csv("../train_dataset (1).csv", index_col="receipt")
  dataset.plot()
  path = 'this.png'
  plt.savefig(path)
  return path
#+end_src

#+RESULTS:
[[file:this.png]]

[[file:~/NextCloud/aadcg_solution.html][solution]]

**** Task

[[https://habr.com/ru/post/457630/][link to the article]]

 Опыт разработки требований к профессиональным качествам data scientist

    Занимательные задачки, Big Data, Математика, Управление персоналом,
    IT-компании

Сегодня практически любой бизнес ощущает потребность в исследовании данных. Data
science не воспринимается как нечто новое. Тем не менее, не для всех очевидно,
каким должен быть нанимаемый специалист.

Данная статья написана не HR-специалистом, а дата сайнтистом, поэтому стилистика
изложения весьма специфична, но в этом есть и преимущество – это взгляд изнутри,
позволяющий понять, какие качества data scientist являются необходимыми для
профессии, для того, чтобы компания могла положиться на такого человека.

Пролог

Пришло время, когда data science стартап вырос из пеленок — число задач для
анализа возросло с непредвиденной скоростью, и эта скорость сразу же перестала
компенсироваться автоматизацией. Стало очевидно, что нужны новые мозги в
команду…

Как мне сначала казалось, человек требовался вполне определенный: всего лишь
обычный дата-что-то-там… программист, аналитик, статистик. Так в чем же
сложность составить список требований?

    “В инженерном деле, если не знаете, что делаете — не стоит этого делать.”
    Ричард Хэмминг


Подошел я к делу как обычно. Достал два листа бумаги. Один озаглавил
«Технические навыки», другой — «Профессиональные качества». После этого возникло
желание полезть на какой-нибудь ресурс, найти там пачку резюме, выписать списки
качеств, выбрать те, что понравятся. Но что-то меня остановило. “Это не мой
способ, — сказал я себе. — Я в этом не разбираюсь. Я разбираюсь в задачах..”

Я попытался пойти от задачи. Задачи у нас простые. Тебе дают нераспарсенную CRM
сомнительного наполнения и просят предсказать объем продаж на пару месяцев
вперед. Совсем просто. Кто угодно справится… Оговорка: если сможет разобраться в
бизнесе клиента. В идеале для этого берется рабочая группа, которая
абстрагируется от всех прочих задач и посвящает себя разбору именно этой. На
входе – желания клиента, на выходе – решение, которое можно проверить, не
вдаваясь в подробности и не дублируя выполненную работу.

Отсюда я сложил первое сколь-нибудь формальное требование – человек должен уметь
взять на себя отдельную задачу и особо никого не дергать до того момента, пока
не будет получено первое грубое решение. Потом это решение можно будет улучшать,
привлекая спецов в помощь. Но на первом этапе задействовать кого-либо еще – это
все равно, что ставить над человеком надсмотрщика. А надсмотрщик может в любой
момент оттолкнуть новичка и начать все делать за него, сделав найм абсолютно
бессмысленным.

Исходя из этого первого требования, я очень быстро заполнил первый лист: знать
python, уметь извлекать информацию из разных источников, хранить информацию,
использовать AWS, знать тервер и статистику, уметь случайные процессы. Чуть
позже добавил туда экономику в базовой версии. В итоге получился список навыков,
необходимый для того, чтобы первое требование выполнялось.

А вот, со списком профессиональных качеств у меня не заладилось. Даже погуглив,
я не нашел каких-либо требований к профессиональным качествам для
data-scientist, которые казались подходящими.

Выплывали либо общие формулировки вида «ответственность», либо под качествами
понимались навыки, что относилось к другому списку.

Собственные же мысли смешивались в кашу, которую было сложно
систематизировать. Глобальное смешивалось с конкретным, применимым только к
определенным задачам. Выносить же одной кучей такие качества, которые были
слишком общими, рядом с качествами, которые впоследствии кандидат мог никогда не
использовать, мне показалось очень неправильным.

Вот где-то тут и родилась идея Задачи. Хороший и элегантный, как мне показалось,
способ откупиться от необходимости философствовать над списками требований, а
заодно собрать нужный список, глядя на ошибки в решениях.

Формулировка Задачи

Предприниматель решил открыть магазин при бадминтонных кортах, чтобы посетителям
не пришлось ездить в супермаркет за воланами (shuttlecock) и ракетками (racket).

На протяжении года предприниматель сохранял все чеки (receipt) от покупок, чтобы
впоследствии понимать, какие решения следует предпринимать для увеличения
прибыли. Информация с чеков содержится в прилагаемом файле train_dataset.csv.

Воланы и ракетки он упаковывал и продавал исключительно наборами трех видов:

    Ракетка и два волана Ракетка и пять воланов Десять воланов


Время от времени предпринимателю приходилось изменять цены с оглядкой на цены в
супермаркете и налоговые ставки.

Магазин и корт работали без выходных и праздников. Поток покупателей несколько
ограничивался из-за того, что на корте допускается присутствие только 4-х
человек, а корт заранее бронируется на сессии длиной два часа, всего на стадионе
три корта. Тем не менее, не проходило ни дня без продажи, поскольку время от
времени на корт приходили либо совсем неподготовленные люди, либо кто-нибудь
рвал ракетку или терял воланы.

Спустя год предприниматель решил устроить распродажу, которая должна продлиться
с первого января по тридцать первое января включительно. Он перераспределил
наборы товаров и назначил им следующие цены:

    Только одна ракетка – 11 долларов 80 центов Пять воланов – 5 долларов 90
    центов Одна ракетка и один волан – 12 долларов 98 центов


Требуется установить размер дохода предпринимателя в январе.

Чувствительность к вероятностям

    “Я верю, что лучшие предсказания основаны на понимании вовлеченных в процесс
    фундаментальных сил.”  Ричард Хэмминг


Задача была составлена в подражание настоящим задачам из жизни, но искусственным
образом, что не скрывалось от кандидатов. А, следовательно, для создания
датасета были применены некие формулы. Пусть, сдобренные случайными переменными,
но формулы. В любом случае, предполагалось, что data scientist имеет возможность
эти формулы обнаружить и использовать для прогнозирования.

Конечно, нельзя отбрасывать и такую возможность, что датасет не даст полной
картины, позволяющей восстановить формулы с нужной точностью. Но на этот случай
в реальной жизни мы придумываем, какой должна быть дополнительная информация, и
откуда бы ее достать.

Вообще, стремление найти «закон мироздания» — это хорошее профессиональное
качество. Умение понять, что искать и где искать – тоже. Господин Хэмминг знал,
о чем говорит. И благодаря ему, в моем списке требований появилась первая
строка:

Способность обнаруживать причинно-следственные связи, описывать их,
формулировать условия, при которых связи могут быть преобразованы в полезную для
бизнеса формулу.

Не случайно здесь я употребил словосочетание «полезную для бизнеса». В моей
личной практике часто оказывалось так, что не ответ к задаче приносил бизнесу
прибыль, а некий побочный результат, который получался за счет открытия каких-то
внутренних зависимостей. В отдельных случаях это приносило стартапу
дополнительные деньги, новые контракты, растило объем ноу-хау и побочных
продуктов.

Поэтому, разбирая присланные мне решения, я внимательно смотрел, как кандидат
будет использовать знание об искусственности датасета, запросит ли он в какой-то
момент добавочную информацию или же докажет достаточность датасета для
выполнения задачи.

Самоуверенность

    “Если какое-то событие привлекает наше внимание, ассоциативная память
    начинает искать его причину, а точнее, активируется любая причина, уже
    хранящаяся в памяти.”  Даниель Канеман


Не скажу, что ассоциативная память плоха. Она – источник и топливо нашей
фантазии. Фантазия позволяет генерировать гипотезы, интуитивно выдвигать
предположения, быстро находить те пары переменных, между которыми возможна
связь.

И она же ставит нам подножку в виде предвзятости подтверждения.

Мы так привыкаем к собственному опыту и собственным знаниям, что начинаем
распространять их на новые ситуации. В мире живого это часто полезно. Скажем,
убеждение, что все змеи ядовиты, спасает больше жизней, чем сомнение в том, что
данная конкретная змея не ядовита. Но в безопасном офисе, имея достаточно
времени, все же любое свое суждение лучше воспринимать как гипотезу.

Датасет к задаче специально был составлен таким образом, что временной интервал
охватывал только год наблюдений. Хорошо, что кандидаты на этапе рассмотрения
графиков выдвигали гипотезу о наличии сезонных колебаний. Плохо, что редко кто
заявлял о необходимости это проверить. И очень плохо, что некоторые без проверки
настаивали на наличии сезонности.

Так я вписал в список качеств следующее:

Критичность мышления, в том числе, в отношении собственного опыта.

Очень хотелось добавить сюда «и знаний», но далее мне показалось, что эта
приписка открывает большую новую тему.

Нейротизм

    “Разработав ту или иную теорию, мы вновь обращаемся к наблюдениям, чтобы
    проверить ее.”  Грегори Мэнкью


В литературе по data science разбираются способы автоматизировать проверку
гипотез. Однако, я редко встречал методические указания по их применению. Из-за
этого, поверите или нет, однажды я запутался между двумя, казалось бы, очень
разными мероприятиями – проверкой статистических гипотез и проверкой модели.

Заодно, что еще сильнее сбивает с толку, упускается из виду разница между
понятиями статистическая гипотеза и гипотеза вообще. Чтобы в нашей статье не
было такой путаницы, позвольте мне для общего понятия гипотеза применить термин
предположение.

В предыдущем параграфе одно такое предположение было выдвинуто в отношении
датасета, а именно, наличие сезонности. Вполне интуитивно можно определить
сезонную компоненту как периодически повторяющуюся. И здесь следует сразу задать
себе вопрос: а сколько раз должна повториться компонента, чтобы ее можно было
считать сезонной? Тем более, можем ли мы на основании периодического повторения
утверждать наличие сезонной компоненты в датасете, временной интервал которого
всего год.

Как уже было сказано, длина интервала была подобрана специально. Я хотел, чтобы
кандидаты имели необходимость и возможность предложить собственные способы
проверки наличия сезонности для рассматриваемой задачи. И это качество я тоже
добавил в список требуемых профессиональных качеств:

Умение проверять предположения стандартными способами и придумывать новые
способы проверки.

Наверное, «придумывать новые способы» звучит слишком громко. Я редко когда
сталкиваюсь с необходимостью придумать что-то новое. Вполне подходит метод
простых соображений, следующих за вопросом «А что, если?».

В красивой статье «Это правильно, но неверно» Александр Черноокий привел примеры
быстрого и почти интуитивного решения для нескольких вероятностных
задач. Подобный механизм, как мне кажется, довольно хорошо подходит и для
проверки предположений.

Сначала подумаем, какого рода сезонность мы хотим найти. Сезонность может быть
внешним фактором, который нам неизвестен, и который олицетворяет некую
паранормальную повторяемость в данных. Описать такую сезонность, не выходя за
рамки датасета, можно выписав отдельно сезонную компоненту и показав степень ее
устойчивости. А еще сезонность может быть спрятана внутри известных
данных. Например, если сезонность влияет на число покупателей, а число
покупателей на объем продаж, то если бы мы знали наперед и полностью, когда
какой покупатель придет, вряд ли нам вообще понадобилась сезонность как
отдельное явление. Стало быть, мы будем искать именно паранормальную сезонность,
поскольку она нам неизвестна и нужна.

Давайте теперь предположим, что такая сезонность на продажи не влияет. Тогда все
колебания объема продаж либо случайны, либо можно найти некую зависимость между
ними и изменениями других переменных. На сколько полно эта зависимость опишет
происходящее? Останется ли вообще место для паранормальной сезонности?

То есть, чтобы проверить наличие сезонности, мы можем найти все зависимости от
известных переменных, и после этого, вычтя из колебаний эти зависимости,
смотреть на остаток. Более того, если разброс остатка будет достаточно мал, то,
возможно, и не будет смысла вообще в поиске паранормальных величин.

Вот мы и получили простой способ проверить наличие сезонности за неимением
достаточно продолжительного интервала данных.

Внимание

    “Наш разум не подготовлен к пониманию редких событий.”  Роберт Баннер


Обратившись к поиску взаимосвязи между двумя величинами, мы первым делом
пытаемся ощутить их взаимное изменение. И нет, пожалуй, способа более простого и
более проработанного, чем линейная регрессия. Она способна помочь составить
мнение о взаимосвязи даже в тех случаях, когда количественное соотношение между
величинами неизвестно. Ну и обладает рядом прочих преимуществ.

И недостатков.

На деле зависимость между двумя величинами далеко не всегда настолько проста,
что ее можно опознать по численным характеристикам. Каким бы красивым ни было
линейное приближение взаимосвязи двух величин, всегда остается возможность, что
мы имеем дело с чем-то более сложным. Английский математик Френсис Энскомб
проиллюстрировал этот феномен четырьмя примерами, которые позже получили
наименование «квартет Энскомба».

Заложить в задачу нечто похожее на квартет Энскомба оказалось хорошей идеей и
весьма простой в реализации. Несмотря на известность феномена, на удочку
попались очень многие кандидаты.

Реализация феномена в задаче выглядела следующим образом. Пусть имеются три
группы клиентов, каждая из которых при покупке реализует некий интерес. Две
группы ведут себя похожим образом, и их поведение выражается в линейной
зависимости между спросом и ценой. Но третья группа поступает иначе. С переходом
цены выше определенного порога покупатели из этой группы резко перестают
покупать более необходимого минимума.

Это явление, довольно распространенное в реальном мире, позволило смоделировать
один из примеров Энскомба и спрятать его среди двух других распределений.

На деле «спрятать» мало подходит к ситуации. Я просто поставил это распределение
рядом с другими, более привычными и понятными. Разница была очевидной на
графиках, как мне казалось, но заметили далеко не все. И особенно интересной
была попытка одного из кандидатов «улучшить» аппроксимацию переходом к полиному
более высокого порядка.

Так я сформулировал очередное требование к профессиональным качествам:

Уметь вычленять значимые наблюдения, строить гипотезы относительно их
значимости.

Импульсивность

    “Измерительный прибор подвергался интенсивному использованию в течение пяти
    лет и прошел через три проверки.”  Тимоти Лири


Ранее я уже описывал ситуацию, когда необъясненные остатки становятся настолько
малы, что их влияние становится неразличимым на фоне тех преимуществ для
бизнеса, которые дает остальная часть модели.

Однако требуется понять, что может скрываться за выражением «настолько малы».

Обычно мир наблюдается и измеряется нами с помощью неких приборов. Простых,
вроде линейки, или сложных, вроде электронного микроскопа. В число сложных
приборов входит и компьютер с установленной на него средой статистического
программирования.

В некотором смысле любое сделанное нами наблюдение или вывод могут
восприниматься как результат измерения. Мы смотрим на условия задачи и измеряем
доход на еще не случившемся временном интервале. Здесь я заменил таинственное и
магическое для многих слово «прогнозируем» на слово «измеряем». В рамках своей
обыденной работы я могу вполне так сказать, поскольку прогноз при достаточно
высоком уровне точности замещается рутинным расчетом.

Но любое измерение не может быть предельно точным. У каждого прибора есть
погрешность измерения, вызванная его несовершенством. И в измерениях необходимо
указывать их точность, для этого вместе с полученным результатом указывается
доверительный интервал.

Указание доверительного интервала есть даже не рекомендация, а необходимость, о
которой часто забывают. Более того, хотя в моих словах прозвучит некая
педантичность, я считаю, что расчет доверительного интервала есть акт
самоуважения, а следующее качество входит в число необходимых качеств для data
scientist:

Аккуратность в соблюдении формальных требований алгоритмов и методов, особенно
когда дело касается расчета доверительных интервалов и проверки необходимых и
достаточных условий.

Пластичность

    “Это положение не вполне верно, но верно в достаточной мере для
    практического применения в большинстве случаев.”  Френсис Энскомб


До сих пор я избегал обсуждения наиболее ярко бросающейся в глаза особенности
этой задачи. Прогнозируемый интервал характеризуется сильным изменением
продаваемого товара. Теперь настало время объяснить, почему это изменение
фигурирует в задаче.

Выше я уже изложил свой взгляд на возможности проверки различных
предположений. Проверка должна быть всегда. Если что-то не может быть проверено,
или способ проверки не известен, то следует изложить различные варианты; они
могут послужить причиной для дальнейших исследований. Но, вместе с тем,
необходимо попробовать максимально описать ситуацию, исходя из известной
информации.

В самом деле, что нам известно о продажах? Есть люди, которые в силу известных и
перечисленных причин производят покупки. Можно почти полностью смоделировать
весь процесс, поскольку мы нашли все зависимости и выяснили, что необъясненный
остаток является нормально распределенным и имеет очень маленькую дисперсию.

Начинают появляться вопросы: а покрывает ли покупаемый объем товара потребности
людей? Что они делают, когда потребность остается неудовлетворенной? Например,
что они делают, если цена на товар, по их мнению, слишком велика? Откуда берется
линейная зависимость спроса?

На самом деле, это вопросы к бизнесу. И их, несомненно, следует задать владельцу
бизнеса как эксперту в своей области. В конце концов, исходный датасет далеко не
всегда полон, а бизнес, даже обладая штатом аналитиков-профессионалов, далеко не
все знает. Собственно, бизнес обращается к data science именно потому, что не
все знает. Но что, если…

Что, если существует проверяемая и непротиворечивая модель, описывающая ситуацию
с использованием только имеющихся у нас известных данных? Это тоже стоит
проверить.

Эпилог

Позвольте составить итоговый перечень выписанных мною профессиональных качеств
data scientist.

    Способность обнаруживать причинно-следственные связи, описывать их,
    формулировать условия, при которых связи могут быть преобразованы в полезную
    для бизнеса формулу.  Критичность мышления, в том числе, в отношении
    собственного опыта.  Умение проверять предположения стандартными способами и
    придумывать новые способы проверки.  Уметь вычленять значимые наблюдения,
    строить гипотезы относительно их значимости.  Аккуратность в соблюдении
    формальных требований алгоритмов и методов, особенно когда дело касается
    расчета доверительных интервалов и проверки необходимых и достаточных
    условий.


В таком собранном виде список мне кажется довольно очевидным. Возможно, потому
что в некоторой мере повторяет список когнитивных искажений. Что, кстати,
наводит меня на мысль о естественной очевидности апостериорных наблюдений. И,
все же, я помню время медитации над вторым пустым листом бумаги и понимаю, что
список не был бы собран без проделанной работы.

Еще интересна мысль о том, что важность некоторого факта для одного человека не
обязательно очевидна для другого. Это хорошо прослеживается по тем решениям
задачи, которые я получил от десятков кандидатов…

Автор: Валерий Кондаков, Co-founder и CTO компании Uninum Соавтор: Павел
Жирновский, Co-founder и CEO компании Uninum

P.S.

Статистика по вакансии на 25/06/19 Дата размещения вакансии: 27/05/19 Всего
просмотров вакансии: 2727 Всего откликов: 94

    Прислали решение задачи, но оно оказалось неверным: 20% Согласились решить
    задачу, но не прислали ответ: 30% Отказ на стадии рассмотрения резюме по
    различным причинам: 45% Прислали решение, близкое к правильному: 5%

*** Artidis AG                                              :ARCHIVE:
**** Test
#+begin_comment
Explain the problem and provide feedback about the most suitable data analysis
path (including machine learning techniques) which will find which feature or
combination of features make these 2 groups the most distinct.
#+end_comment

This is a supervised learning problem - we're given both the features and the
targets. It's also a (binary) classification problem since the targets are discrete.

***** Data preparation

- Read the data as a csv file (I like pandas)

#+begin_src python :session
  import pandas as pd

  data = pd.read_csv("~/Downloads/spot-rCsvSpotProb.csv")
  data.head()
#+end_src

#+RESULTS:
:     ID     SI      A      C    StF                    SoF       N     MG  BC
: 0   64  28-m1  1.01%  0.00%  0.00%                  0.00%  95.34%  0.00%   5
: 1  108  03-m1  2.93%  0.00%  0.00%                  0.00%  88.57%  0.00%   5
: 2  282  01-m1  0.33%  0.00%  0.00%                  0.00%  77.32%  0.00%   5
: 3  248  27-m1  7.47%  0.00%  0.00%  1.59890819253202e-315  74.21%  0.00%   5
: 4  140  01-m3  1.19%  0.00%  0.00%                  0.00%  72.89%  0.00%   5

#+begin_src python :session
  data.dtypes
#+end_src

#+RESULTS:
#+begin_example
ID      int64
SI     object
A      object
C      object
StF    object
SoF    object
N      object
MG     object
BC      int64
dtype: object
#+end_example

- Get rid of the ID column (it's not a feature);

- Convert the features into floats (getting rid of "%" and converting from
  string);

- Our feature space shares the same scale, since all values \(\in [0,100]\);

- Some values are almost 0, but not really 0. Assumming it's a computational
  artifact, I'd turn those into a 0 if \(< 10^{-16}\);

- Separate the features from the targets (while saving the labels in a list) and
  convert those into numpy arrays since scikit-learn requires it;

- Split the data into training and testing sets (trivially done with
  scikit-learn).

Note: I'm not sure what to do about SI. In the e-mail, I was told: /there are 6
features - A, C, StF, SoF, N, MG/. So, I guess I'd just drop that
column. Categorical features needs to be encoded properly (one hot encoding).

***** Random Forest

At this point, I'd use the [[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_][random forest algorithm]].

The [[https://en.wikipedia.org/wiki/F1_score][F1 score]] would be a measure to access the accuracy of the
classification. Minor adjustments to the hyper-parameters would be made to the
model.

Once the classification would be satisfactory (after cross validation), I'd use
the [[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_][features_importances_]] function to answer the posed question:

*Which feature(s) make the 2 groups most distinct?*

***** Conclusion

I've described what would the most simple and natural approach, in my opinion.

Turning the above steps into a script would be quite straight forward.

*** [[https://assaia.com/][Assaia International AG]]                                    :ARCHIVE:

- State "WAITING"    from              [2019-07-20 Sat 00:07] \\
  Hi Andre,

  I've talked to Dmitry and we have really tried hard to find a match for your
  skills in our company but for now we don't see a great match unfortunately. I
  would certainly love to keep in touch with you! If you could remind us about
  yourself back in several months, when we have a larger team and more open
  positions, that would be great.

  I wish you a fantastic weekend!
  Nikolay

[[https://www.linkedin.com/in/nkobyshev][Nikolay]]
nk@assaia.com
dc@assaia.com

*** Coinweb                                                       :ARCHIVE:

[[file:~/aboutme.org][about me]]

<2019-08-29 Thu 10:00> Juan Pablo
<2019-08-26 Mon 9:00-10:00> Juan Pablo
<2019-08-22 Thu 9:00-10:00> Roger
<2019-08-21 Wed 14:30-15:30> Alejandro

- data structures (hash tables and binary tree)
- functors and monads (haskell)
- symmetric vs. asymmetric encryption
- SSL certificates
- quicksort
- inner and outer join
- primary key (database)
- database normalization
- algorithms
- property based testing

** Vestas                                                          :ARCHIVE:
:PROPERTIES:
:START: [2018-07-30]
:FINISH: [2019-07-29]
:END:

[[https://blog.juliobiason.net/thoughts/things-i-learnt-the-hard-way/][what not to do]]

*** People
**** Emer
:PROPERTIES:
:EMAIL: emer@emer.me
:GITHUB:
:NOTES:
:END:

**** Miguel Borges (HPC)
:PROPERTIES:
:EMAIL: borges.miguel@gmail.com
:NOTES:
:END:

**** Carlos (FEUP)
:PROPERTIES:
:EMAIL: cvrodrigues@gmail.com
:NOTES:
:END:

**** Richard
:PROPERTIES:
:EMAIL: richardjmcsherry@yahoo.co.uk
:NOTES:
:END:

**** DARFE
:PROPERTIES:
:EMAIL: ferreira.d4.r@gmail.com
:NOTES:
:END:

**** João Fortes
:PROPERTIES:
:EMAIL: joao.fortes@sapo.pt
:NOTES:
:END:

*** Projects                                                      :ARCHIVE:
**** PSE2
***** Benchmark
#+begin_src python
  import logging
  import json
  import yaml
  import os
  import pandas as pd
  import benchmark.auxiliar_endpoints as ae


  class Benchmark:
      """
      Populates the benchmark folder to run pse2.vsu. The starting point for this
      script is the list_benchmark.csv file which contains meta data for each
      benchmark project. The script translates that dataframe into a benchmark
      folder suitable to run PSE2.

      """

      def __init__(self, proj):
          super(Benchmark, self).__init__()
          self.proj = proj
          self.layout_id = self.proj['LayoutID']
          self.proj_id = proj['VSUProjectID']
          self.proj_path = root + self.proj_id

      def vsu(self):
          """
          - Writes vsu_inputs.ac
          - Downloads met mast data
          """
          json_obj = dict.fromkeys(['Meta', 'Meas'])

          irrel_cols = ['VSUProjectID', 'LayoutID', 'Relationship']
          json_obj['Meta'] = dict(proj.drop(labels=irrel_cols))
          json_obj['Meta']['Latitude'] = \
              ae.compute_site_center_and_corners(self.layout_id)[0][0]
          json_obj['Meta']['Longitude'] = \
              ae.compute_site_center_and_corners(self.layout_id)[0][1]
          json_obj['Meta']['site_corners'] = \
              ae.compute_site_center_and_corners(self.layout_id)[1]
          json_obj['Meta']['CrmId'] = self.proj_id

          try:
              masts = ae.nonltc_masts(self.layout_id)
              logging.info(
                  f'ProjectID {self.proj_id} got list non-LTC met stations')
          except:
              logging.error(
                  f'ProjectID {self.proj_id} failed to get list non-LTC met stations')
              pass
          dict_meas = {}
          for i, mast_info in masts.iterrows():
              mast_id = mast_info['MetStationsId']
              lat, lon = mast_info['Latitude'], mast_info['Longitude']
              dict_meas[mast_id] = {'Latitude': lat, 'Longitude': lon}
              meas_path = os.path.join(
                  self.proj_path, 'vsu', 'meas', 'met_mast_' + mast_id)
              os.makedirs(meas_path, exist_ok=True)
              # meso
              meso_path = os.path.join(
                  self.proj_path, 'vsu', 'meso', 'meso_point_' + mast_id)
              os.makedirs(meso_path, exist_ok=True)

              cond1 = os.path.isfile(meas_path + '/raw.csv') is False
              cond2 = os.path.isfile(meas_path + '/ws_filtered.csv') is False
              if cond1 or cond2:
                  try:
                      ae.download_masts(mast_id, meas_path)
                      logging.info(
                          f'ProjectID {self.proj_id} MastID {mast_id} download mast data')
                  except:
                      logging.error(
                          f'ProjectID {self.proj_id} MastID {mast_id} failed to download mast data')
                      pass
              else:
                  logging.info(
                      f'ProjectID {self.proj_id} MastID {mast_id} mast data already existed')
          json_obj['Meas'] = {'MetMasts': dict_meas}
          with open(self.proj_path + '/vsu_inputs.json', 'w') as json_file:
              json.dump(json_obj, json_file)
              logging.info(
                  f'ProjectID {self.proj_id} wrote vsu.json file')

      def ac(self):
          """
          - Writes ac_inputs.json
          - Dumps
          """
          json_obj = dict.fromkeys(['Meta', 'pfmp', 'power_ct'])
          meta_keys = ['CrmId',
                       'site_borders',
                       'n_config',
                       'max_capacity',
                       'max_pad',
                       'max_tip_height',
                       'ud_site_wind',
                       'layout',
                       'ud_pad_wind',
                       'ud_site_wind',
                       'ud_spacing_min',
                       'ud_spacing_max',
                       'park_lifetime',
                       'rsf_filename']
          meta = dict.fromkeys(meta_keys)
          farm = ae.farm_detailed(self.layout_id)
          nr_turb = len(farm)
          meta['n_case'] = 1
          meta['CrmId'] = self.proj_id
          meta['site_borders'] = ae.compute_site_borders(self.layout_id)
          meta['max_pad'] = nr_turb
          meta['max_capacity'] = farm['NominalPowerKw'].sum()
          meta['max_tip_height'] = farm['HubHeight'].max() + \
              (farm['RotorDiameter'].max() / 2)
          meta['park_lifetime'] = 20
          meta['rsf_filename'] = 'rsf.nc'
          param = ['Latitude',
                   'Longitude',
                   'wtg_id',
                   'pad_type']
          meta['layout'] = ae.farm_detailed(self.layout_id)[param].to_dict(
              'records')+ae.ext_turb(self.layout_id)[param].to_dict('records')
          json_obj['Meta'] = meta

          pfmp = dict.fromkeys('0')
          pfmp['0'] = ae.pfmp(self.layout_id).to_dict(orient='index')
          json_obj['pfmp'] = pfmp

          pc_ids = ae.pfmp(self.layout_id)[
              'PowerCurveIdentifier'].apply(str).values.tolist()
          power_ct = dict.fromkeys(pc_ids)
          for pc_id in pc_ids:
              curves = dict.fromkeys(['power_curve',
                                      'ct_curve',
                                      'ud_power_curve',
                                      'ud_ct_curve'])
              curves['power_curve'] = ae.download_curves(pc_id)[0]
              curves['ct_curve'] = ae.download_curves(pc_id)[1]
              power_ct[pc_id] = curves
          json_obj['power_ct'] = power_ct

          with open(self.proj_path + '/ac_inputs.json', 'w') as json_file:
              json.dump(json_obj, json_file)
              logging.info(f'ProjectID {self.proj_id} wrote ac.json file')


  config = yaml.safe_load(open('../pse2/config.yml'))
  root = config['benchmark']['benchmark_path']
  projects = pd.read_csv(config['benchmark']['benchmark_list'])
  n_projs = len(projects)

  logging.basicConfig(level=logging.INFO,
                      format='%(asctime)s  %(levelname)8s   %(message)s',
                      filename=root + 'benchmark.log',
                      filemode='w',
                      datefmt='%d-%b-%y %H:%M:%S')

  for idx, proj in projects.iterrows():
      proj_obj = Benchmark(proj)
      logging.info(
          f'ProjectID {proj_obj.proj_id} started ({idx+1}/{n_projs})')
      # proj_obj.vsu()
      proj_obj.ac()
      logging.info(f'ProjectID {proj_obj.proj_id} finished \n')

#+end_src
****** Mv rsf
:PROPERTIES:
:ID:       88f6ad79-b8ff-43a5-a8eb-64ce354b8c5a
:ARCHIVE_TIME: 2019-05-03 Fri 14:22
:ARCHIVE_FILE: ~/NextCloud/org/work.org
:ARCHIVE_OLPATH: Vestas/PSE2/Benchmark
:ARCHIVE_CATEGORY: work
:ARCHIVE_TODO: NOT TODO
:END:
- State "NOT TODO"   from "WAITING"    [2019-05-03 Fri 14:22] \\
  Хер знает
- State "WAITING"    from "TODO"       [2019-03-21 Thu 13:42] \\
  Rerun vsu
ls -d */ | xargs -i cp {}rsf.h5 ../benchmark.v2/{}file_new_name

****** Restructure Wind Service
:PROPERTIES:
:ID:       29176eab-dfb7-4b2b-8c26-8857ea18885d
:ARCHIVE_TIME: 2019-05-07 Tue 09:22
:ARCHIVE_FILE: ~/NextCloud/org/work.org
:ARCHIVE_OLPATH: Vestas/PSE2/Benchmark
:ARCHIVE_CATEGORY: work
:ARCHIVE_TODO: NOT TODO
:END:
- State "NOT TODO"   from "WAITING"    [2019-03-21 Thu 09:40]
- State "WAITING"    from              [2019-03-13 Wed 14:50] \\
  waiting for ewmac and jupmp opinion

****** Mv rsf
:PROPERTIES:
:ID:       88f6ad79-b8ff-43a5-a8eb-64ce354b8c5a
:ARCHIVE_TIME: 2019-05-07 Tue 09:23
:ARCHIVE_FILE: ~/NextCloud/org/work.org
:ARCHIVE_OLPATH: Vestas/PSE2/Benchmark
:ARCHIVE_CATEGORY: work
:ARCHIVE_TODO: NOT TODO
:END:
- State "NOT TODO"   from "WAITING"    [2019-05-03 Fri 14:22] \\
  Хер знает
- State "WAITING"    from "TODO"       [2019-03-21 Thu 13:42] \\
  Rerun vsu
ls -d */ | xargs -i cp {}rsf.h5 ../benchmark.v2/{}file_new_name

****** Meas stats
:PROPERTIES:
:ARCHIVE_TIME: 2019-05-07 Tue 09:23
:ARCHIVE_FILE: ~/NextCloud/org/work.org
:ARCHIVE_OLPATH: Vestas/PSE2/Benchmark
:ARCHIVE_CATEGORY: work
:ARCHIVE_TODO: DONE
:END:
- State "DONE"       from              [2019-04-05 Fri 09:43]

#+begin_src python
  import matplotlib as mpl
  import matplotlib.pyplot as plt
  import re
  import datetime
  import numpy as np
  import os
  import pandas as pd

  results_dir = '/ifs/dm/cfd/app/PSE2/benchmark_timdc.v2.hres'


  def meas_metrics(projid,
                   results_dir='/ifs/dm/cfd/app/PSE2/benchmark_timdc.v2.hres'):
      meas_dir = os.path.join(results_dir, projid, 'vsu', 'meas')
      log_path = os.path.join(results_dir, projid, 'logs', 'pse2.vsu.log')
      ids = [item[9:] for item in os.listdir(meas_dir)]
      d = dict.fromkeys(ids)
      for id in ids:
          path = os.path.join(meas_dir, 'met_mast_' + id, 'raw.csv')
          ten_min_blocks, signals = pd.read_csv(
              path, header=7).set_index('Unnamed: 0').shape
          time = meas_time_eval(id, log_path)
          d[id] = [ten_min_blocks, signals, time]
      return d


  def meas_time_eval(id, log_path):
      FMT = "%Y-%m-%d %H:%M:%S"
      with open(log_path) as f:
          meas = [line
                  for line in f.readlines()
                  if re.findall(id, line)
                  and
                  (re.findall('Start Filtering', line)
                   or
                   re.findall('Finish met', line))]
      if len(meas) == 2:
          return (datetime.strptime(meas[1][:19], FMT)
                  -
                  datetime.strptime(meas[0][:19], FMT)).seconds
      else:
          return np.nan


  l = []
  for id in os.listdir(results_dir):
      l.append(list(meas_metrics(id, results_dir).values()))
  stats = np.array(sum(l, []))
  mpl.rcParams['figure.facecolor'] = 'FFFFFF'
  mpl.rcParams['figure.figsize'] = (15, 8)
  x = stats[:, 0]
  y = stats[:, 1]
  z = stats[:, 2]
  plt.scatter(y, x, c=z/60)
  plt.colorbar().set_label('runtime (min)')
  plt.xlabel('# signals')
  plt.ylabel('# 10 min blocks')
  plt.savefig('~/meas_plot.pdf')
#+end_src

****** XML files
:PROPERTIES:
:ID:       e9890118-1468-4add-9dbc-5a1d9e3884df
:ARCHIVE_TIME: 2019-05-07 Tue 09:23
:ARCHIVE_FILE: ~/NextCloud/org/work.org
:ARCHIVE_OLPATH: Vestas/PSE2/Benchmark
:ARCHIVE_CATEGORY: work
:ARCHIVE_TODO: DONE
:END:
- State "DONE"       from "TODO"       [2019-04-01 Mon 13:23]

#+begin_src python
  def xml_fn(xml_dir):
      """Returns a list of the file name's excluding the extension

      :param xml_folder:
      :returns:
      :rtype:

      """
      return [xml_fn[:-4] for xml_fn in os.listdir(xml_dir)]


  def matching_blocks(string, list_strings, n):
      if len(list_strings) == 1:
          return list_strings
      # return ['.'.join(s.split('.')[n:])
      #         for s in list_strings
      #         if string.split('.')[:n] == s.split('.')[:n]]
      else:
          return [s
                  for s in list_strings
                  if string.split('.')[:n] == s.split('_')[:n]]


  def lcs_metric(string, list_strings):
      if len(list_strings) == 1:
          return list_strings
      else:
          list_strings = np.array(list_strings)
          metric = np.array(
              [lcs(string, s)
               for s in list_strings])
          return list_strings[metric == metric.max()]


  def transf_string(string):
      return re.sub('[^a-z A-Z 0-9]', '.', string).replace('..', '.').split('.')[3:]


  def intersection_metric(string, list_strings):
      if len(list_strings) == 1:
          return list_strings
      else:
          metric = np.array(
              [len(set(transf_string(string))
                   &
                   set(transf_string(s)))
               for s in list_strings])
          return list_strings[metric == metric.max()]


  def lcs(X, Y):
      """https://www.geeksforgeeks.org/longest-common-subsequence-dp-4/

      :param X:
      :param Y:
      :returns:
      :rtype:

      """
      # find the length of the strings
      m = len(X)
      n = len(Y)

      # declaring the array for storing the dp values
      L = [[None]*(n + 1) for i in range(m + 1)]

      """Following steps build L[m + 1][n + 1] in bottom up fashion
       Note: L[i][j] contains length of LCS of X[0..i-1]
       and Y[0..j-1]"""
      for i in range(m + 1):
          for j in range(n + 1):
              if i == 0 or j == 0:
                  L[i][j] = 0
              elif X[i-1] == Y[j-1]:
                  L[i][j] = L[i-1][j-1]+1
              else:
                  L[i][j] = max(L[i-1][j], L[i][j-1])

      # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1]
      return L[m][n]

#+end_src

rename 's/\.extension//' *.extension
[[https://en.wikipedia.org/wiki/Hamming_distance][String distances]]

|-----------+----|
| # matches |  # |
|-----------+----|
|         1 | 42 |
|         2 | 43 |
|         3 |  2 |
|         4 | 15 |
|-----------+----|

****** External Turbines
:PROPERTIES:
:ID:       81f254ea-2abf-455f-87c2-92c88296afba
:ARCHIVE_TIME: 2019-05-07 Tue 09:23
:ARCHIVE_FILE: ~/NextCloud/org/work.org
:ARCHIVE_OLPATH: Vestas/PSE2/Benchmark
:ARCHIVE_CATEGORY: work
:ARCHIVE_TODO: DONE
:END:
- State "DONE"       from "WAITING"    [2019-03-20 Wed 16:05]
- State "WAITING"    from              [2019-03-15 Fri 09:02] \\
  Waiting for a dataset from VSU db

#+NAME: T2
| CrmID                                |
| 6eb3b1c4-63c3-4952-bb32-7e369eb84cbf |
| f3fa3263-313c-4496-87eb-131daa5d7c61 |
| 0979ca81-1634-449a-8039-265a7ec20f61 |
| e8660ef8-ff52-49be-9247-32c6fa5e6f43 |
| d91e72ec-5f95-468d-876a-358674d1cc92 |
| f6e060a5-0dc0-4d03-acf3-38c1c88ccdcb |
| fc4ced0c-6edd-4ef5-8201-554705cc5e0b |
| 0e41d2bb-1dbf-4f60-808b-5ea6a3435b06 |
| 0869b498-0e7c-488d-b319-63f02bcb6fac |
| e8ccdc32-4f2a-4ef8-a748-70d9822b6f85 |
| e601fc7f-6750-46db-aad3-7147b75ecd2c |
| c68963f7-a1c4-4ad0-ba01-75cbd34323ca |
| 9fc62893-bbc2-4975-91a0-9ef82bd1d824 |
| 87910d9f-2688-4b0b-8de6-b8176ff825ad |
| d053c038-529c-4027-9910-bca9c2c31443 |
| c5dbe710-7e7b-4838-af48-d423b71ae015 |
| a1a5f822-526e-4853-b8a5-fb0fd26ff258 |
| 565614cc-de78-45fe-af8a-0329a5378324 |
| 8902a43e-200e-4ef5-9299-7fa76b0d8273 |
| c3c6e81b-03a0-46fb-a20c-bf1078f63d64 |
| 200247e5-309a-4191-ac4e-1c58cf72aa1e |
| fe750d15-1d49-40be-abc8-49800cd8ce49 |
| c4036bb9-b4d3-449f-91b9-86c3be1baa6c |
| fc44e236-15df-4847-9e11-ae8b1fea04b1 |
| e4b4cf9d-a6b5-4aea-9d7f-c14f6bd0b5f5 |
| 9f2dfc89-6419-47c6-b3a8-90f818e4af88 |
| 9fe9b703-0a40-40d7-9ae8-e8c8dc5976d1 |
| 4e2a2113-c370-4483-a0ac-e93da6baac03 |
| 75a5a608-3183-4465-9839-5723aa058640 |
| f90abf10-f7a2-47fc-843e-f00df33c1b60 |
| e28566f6-484e-4f3f-99bb-42b1e68ae411 |
| 081c3621-3acd-48b2-8f86-4661cb927521 |
| 986b30f3-f949-4b07-98b4-768dbf614a8e |
| 4c02b49c-6c92-4d43-bab1-adbc1df503ee |
| 5388642a-bd92-4a00-a4d3-cf10b9885a70 |
| d7e39c84-17d7-4059-a09b-3d240eef5e6b |
| 2d2d6ed4-41c2-4889-9272-7f1bb4f6c27d |
| f48d46b5-c2e6-4a1f-8e06-b75401f70dfe |
| a32abb90-d32e-4062-8982-71bf0f5c7dc8 |
| e8ccff81-402d-46fa-aef7-45ee58d5580a |
| 18301f15-d509-4f6d-8c41-45f65111df61 |
| a205d780-be34-4a7f-909c-a28bdfbf6f04 |
| 1c9c7e25-f47a-4e29-bef3-19da8406db63 |
| d5ce0867-e85b-4145-ba98-76da98226584 |
| 61356664-a203-4652-80f4-3693ffe6982f |
| 41f4ee37-3c46-4cc6-99c0-33439297ccfa |
| 0fdeb110-ddab-41ca-ae4c-30e786516c9a |
| 59c3e219-0f72-4573-bf49-9bcf2831eb1f |
| 1fa818fa-1738-43d6-951f-4864fc9d3d94 |
| 780be8d2-504a-4858-9694-b8665ae98983 |
| 2e42ec88-bb33-4cf1-8730-f8bc61b23337 |
| 7c55d3f6-f9d2-42a2-8fa6-605d50b39d61 |
| 4a6077f7-4ce0-4ac6-af05-13afe46f88ae |
| 306383c2-3cd6-4cfb-93d4-a52fe062019e |
| 69083277-1eca-4695-92bb-0e1328b56e44 |
| 25213859-d726-42b0-a20e-41803623c497 |
| 8b7c7bfb-22c6-496a-9de7-2ffa4b3e8e40 |
| a3338fea-383b-403b-bf51-99a95aa8b147 |
| 5f733bbe-9b06-4e13-8b73-fccc5f329dfe |
| 52b93fd8-5090-4aa3-91c5-e45053cef8de |
| ee7c395c-4c3c-46c9-94e8-737cd0fb158c |
| b4b9bdf5-c0f2-4f9f-a979-2eb697e54ce8 |
| accdfa2a-6678-4cce-8016-2cf12489cecd |
| 54ca7238-510d-4767-826e-4ebf5f4f5a36 |
| de34dbe2-3050-4ddd-ad89-401c72718435 |
| 8b0fb296-6a40-4e31-8d55-e62acc920a38 |
| e301c52e-01df-41a7-9a8a-0fb7f28f43de |
| e80506c7-5350-48c3-9271-28f1ec6eaee6 |
| ad2c99e4-b7c5-4f99-98bf-b314d3a9f2af |
| 806c84cf-7b7b-45b7-80ce-b3225377b491 |
| ab0b13f7-d052-43cb-a5aa-c86a47a6217d |
| d73058c4-2961-49b3-8775-1850550f1abf |
| bbe9c3c4-4523-4fd4-97db-1fd0e709e7f8 |
| f46c225d-0442-4a4b-9c04-b5a8e35eb8e2 |
| d6293c21-9025-4afe-8b13-34fbf703a066 |
| 41498a35-7ba9-4c11-8e13-4be111271c93 |
| aabdf0e8-d335-4e3f-8fb1-4c1eb088c892 |
| f400b943-0159-4f2f-a36c-e30e0c4289be |
| 8beeeb10-0fc5-425c-a1cc-744a2c60c78a |
| 0410f736-9499-43aa-b974-baa1f0151621 |
| 74b112ae-0869-4b26-b09c-bac293443a0f |
| 91f0a8b6-86d5-40c7-ad17-61e0dbc6771f |
| 12b0ef60-9cc9-41c7-9ae6-61f093998c4e |
| f81a77ca-f2b4-4f3a-bacb-cb8227810f2e |
| 0e7a746c-2017-4760-b474-90b20640ed92 |
| b31fe844-a7d6-4785-8dc4-07dda9f87b03 |
| db5adca0-0e3e-4b7c-a088-f78d0be808c0 |
| 0a28bc48-c6c4-460f-9c46-f7a20a980caf |
| 294eb348-131b-4dd2-9069-2715414b9c26 |


#+NAME: T1
|  # | Name                                | Country        | project exists? | # external turbines |
|  0 | Benchmark_West_Coast_One            | South_Africa   | True            |                   0 |
|  1 | Benchmark_EBWPC_Burgos              | Philippines    | True            |                   0 |
|  2 | Benchmark_Pintado_Cofusa            | Uruguay        | True            |                   0 |
|  3 | Benchmark_Noblesfontein             | South_Africa   | True            |                   0 |
|  4 | Benchmark_Pagow                     | Poland         | True            |                   0 |
|  5 | Benchmark_Los_Vientos_IV            | United_States  | True            |                 155 |
|  6 | Benchmark_Cernavoda                 | Romania        | True            |                   0 |
|  7 | Benchmark_Florida_II                | Uruguay        | True            |                  21 |
|  8 | Benchmark_Bardy                     | Poland         | True            |                   0 |
|  9 | Benchmark_Eppodumvenran_Beta        | India          | True            |                   0 |
| 10 | Benchmark_Tsitsikamma               | South_Africa   | True            |                   0 |
| 11 | Benchmark__Kalkriese_               | Germany        | True            |                   0 |
| 12 | Benchmark_Ylivieska                 | Finland        | True            |                   0 |
| 13 | Benchmark_Yeong_yang                | South_Korea    | True            |                  26 |
| 14 | Benchmark_Kalajoki_II               | Finland        | True            |                   0 |
| 15 | Benchmark_Avila_Oeste               | Spain          | True            |                   0 |
| 16 | Benchmark_Origin                    | United_States  | True            |                   0 |
| 17 | Benchmark_Fraisthorpe               | United_Kingdom | True            |                   0 |
| 18 | Benchmark_Pioneer_Prairie_I-II      | United_States  | True            |                   7 |
| 19 | Benchmark_Shin-Izumo                | Japan          | True            |                   0 |
| 20 | Benchmark_Pestera                   | Romania        | True            |                   0 |
| 21 | Benchmark_Eolien_Catalan            | France         | True            |                   9 |
| 22 | Benchmark_Amasya                    | Turkey         | True            |                   0 |
| 23 | Benchmark_Corbera_y_Vilalba         | Spain          | True            |                  21 |
| 24 | Benchmark_Linowo                    | Poland         | True            |                   0 |
| 25 | Benchmark_Amliden                   | Sweden         | False           |                 NaN |
| 26 | Benchmark_Coonooer_Bridge           | Australia      | True            |                   0 |
| 27 | Benchmark_Three_Mile_Canyon         | United_States  | True            |                   0 |
| 28 | Benchmark_Timber_Road_II            | United_States  | True            |                 158 |
| 29 | Benchmark_Bothe                     | India          | True            |                  24 |
| 30 | Benchmark_TERNA-Agios_Georgios      | Greece         | True            |                   9 |
| 31 | Benchmark_El_Porvenir               | Mexico         | True            |                   0 |
| 32 | Benchmark_Roosevelt                 | United_States  | False           |                 NaN |
| 33 | Benchmark_Little_Elk                | United_States  | True            |                  93 |
| 34 | Benchmark_Ostaszewo                 | Poland         | True            |                   0 |
| 35 | Benchmark_Bandirma                  | Turkey         | True            |                   0 |
| 36 | Benchmark_Ogorje                    | Croatia        | True            |                   0 |
| 37 | Benchmark_Bingham                   | United_States  | True            |                   0 |
| 38 | Benchmark__Beregovaya_and_Stavki_   | Ukraine        | True            |                   0 |
| 39 | Benchmark_Brotorp_                  | Sweden         | True            |                   9 |
| 40 | Benchmark_Suloglu                   | Turkey         | True            |                   0 |
| 41 | Benchmark_Macho_Springs             | United_States  | True            |                   0 |
| 42 | Benchmark_Magic_Valley              | United_States  | False           |                 NaN |
| 43 | Benchmark_Gebeleisis                | Romania        | True            |                   7 |
| 44 | Benchmark__Botievo_                 | Ukraine        | True            |                  30 |
| 45 | Benchmark_Musselroe                 | Australia      | True            |                   0 |
| 46 | Benchmark_Route_66                  | United_States  | True            |                   0 |
| 47 | Benchmark_Lake_Bonney_Stage_2_and_3 | Australia      | True            |                  69 |
| 48 | Benchmark_Korytnica                 | Poland         | True            |                   0 |
| 49 | Benchmark_Woolnorth_Studland_Bay    | Australia      | True            |                  37 |
| 50 | Benchmark_Glenay                    | France         | True            |                  15 |
| 51 | Benchmark_Pleasant_Valley           | United_States  | True            |                   0 |
| 52 | Benchmark_Samdal                    | South_Korea    | True            |                   0 |
| 53 | Benchmark_Milo                      | United_States  | True            |                 125 |
| 54 | Benchmark_Tafila                    | Jordan         | False           |                 NaN |
| 55 | Benchmark_Goinj_Powerica            | India          | True            |                   0 |
| 56 | Benchmark_Los_Vientos_III           | United_States  | True            |                 155 |
| 57 | Benchmark_Carpinone                 | Italy          | True            |                  45 |
| 58 | Benchmark_Slate_Creek               | United_States  | True            |                   0 |
| 59 | Benchmark_Fallago_Rig               | United_Kingdom | True            |                   0 |
| 60 | Benchmarker_Collgar                 | Australia      | True            |                   0 |
| 61 | Benchmark_Nojoli                    | South_Africa   | True            |                 132 |
| 62 | Benchmark_Burgos_Este               | Spain          | True            |                   0 |
| 63 | Benchmark__Munderfing_              | Austria        | True            |                   0 |
| 64 | Benchmark_Banie_and_Kozielice       | Poland         | True            |                   0 |
| 65 | Benchmark_Iwata                     | Japan          | True            |                   1 |
| 66 | Benchmark_Phu_Lac                   | Vietnam        | True            |                  20 |
| 67 | Benchmark_Vandhiya-Jangi            | India          | True            |                 133 |
| 68 | Benchmark_Nooriabad                 | Pakistan       | True            |                 197 |
| 69 | Benchmark_Los_Vientos_V             | United_States  | True            |                 200 |
| 70 | Benchmark_Renaico_I                 | Chile          | True            |                  42 |
| 71 | Benchmark_Fabodliden                | Sweden         | True            |                   0 |
| 72 | Benchmark_Grassridge                | South_Africa   | True            |                   0 |
| 73 | Benchmark_Horse_Butte               | United_States  | True            |                   0 |
| 74 | Benchmark_Longhorn_North            | United_States  | True            |                   0 |
| 75 | Benchmark_Rawson                    | Argentina      | False           |                 NaN |
| 76 | Benchmark_La_Croix_Benjamin         | France         | True            |                  18 |
| 77 | Benchmark__LosBuenosAires_          | Chile          | False           |                 NaN |
| 78 | Benchmark_Facaeni_EDPR              | Romania        | True            |                   0 |
| 79 | Benchmark_Waterloo                  | Australia      | True            |                   6 |
| 80 | Benchmarker_Mac_Arthur              | Australia      | True            |                   0 |
| 81 | Benchmark_Taralga                   | Australia      | True            |                   0 |
| 82 | Benchmark_Fowler                    | United_States  | True            |                 238 |
| 83 | Benchmark_Mahinerangi               | New_Zealand    | True            |                   0 |
| 84 | Benchmark_Tararua_III               | New_Zealand    | True            |                 200 |
| 85 | Benchmark_Macambira                 | Brazil         | True            |                   0 |
| 86 | Benchmark_Coulommes_et_Marqueny     | France         | False           |                 NaN |
| 87 | Benchmark_Castellaneta_EDPR         | Italy          | True            |                  40 |
| 88 | Benchmark_Sherbino_Mesa             | United_States  | True            |                   0 |


#+BEGIN_SRC emacs-lisp :var t1=T1 t2=T2 :colnames no
  (cl-mapcar #'append t1 t2)
#+END_SRC

#+RESULTS:
|----+--------------------------------------+-------------------------------------+----------------+--------+----------|
|  # | CrmID                                | Name                                | Country        | trust? | ext_turb |
|----+--------------------------------------+-------------------------------------+----------------+--------+----------|
|  0 | 6eb3b1c4-63c3-4952-bb32-7e369eb84cbf | Benchmark_West_Coast_One            | South_Africa   | True   |        0 |
|  1 | f3fa3263-313c-4496-87eb-131daa5d7c61 | Benchmark_EBWPC_Burgos              | Philippines    | True   |        0 |
|  2 | 0979ca81-1634-449a-8039-265a7ec20f61 | Benchmark_Pintado_Cofusa            | Uruguay        | True   |        0 |
|  3 | e8660ef8-ff52-49be-9247-32c6fa5e6f43 | Benchmark_Noblesfontein             | South_Africa   | True   |        0 |
|  4 | d91e72ec-5f95-468d-876a-358674d1cc92 | Benchmark_Pagow                     | Poland         | True   |        0 |
|  5 | f6e060a5-0dc0-4d03-acf3-38c1c88ccdcb | Benchmark_Los_Vientos_IV            | United_States  | True   |      155 |
|  6 | fc4ced0c-6edd-4ef5-8201-554705cc5e0b | Benchmark_Cernavoda                 | Romania        | True   |        0 |
|  7 | 0e41d2bb-1dbf-4f60-808b-5ea6a3435b06 | Benchmark_Florida_II                | Uruguay        | True   |       21 |
|  8 | 0869b498-0e7c-488d-b319-63f02bcb6fac | Benchmark_Bardy                     | Poland         | True   |        0 |
|  9 | e8ccdc32-4f2a-4ef8-a748-70d9822b6f85 | Benchmark_Eppodumvenran_Beta        | India          | True   |        0 |
| 10 | e601fc7f-6750-46db-aad3-7147b75ecd2c | Benchmark_Tsitsikamma               | South_Africa   | True   |        0 |
| 11 | c68963f7-a1c4-4ad0-ba01-75cbd34323ca | Benchmark__Kalkriese_               | Germany        | True   |        0 |
| 12 | 9fc62893-bbc2-4975-91a0-9ef82bd1d824 | Benchmark_Ylivieska                 | Finland        | True   |        0 |
| 13 | 87910d9f-2688-4b0b-8de6-b8176ff825ad | Benchmark_Yeong_yang                | South_Korea    | True   |       26 |
| 14 | d053c038-529c-4027-9910-bca9c2c31443 | Benchmark_Kalajoki_II               | Finland        | True   |        0 |
| 15 | c5dbe710-7e7b-4838-af48-d423b71ae015 | Benchmark_Avila_Oeste               | Spain          | True   |        0 |
| 16 | a1a5f822-526e-4853-b8a5-fb0fd26ff258 | Benchmark_Origin                    | United_States  | True   |        0 |
| 17 | 565614cc-de78-45fe-af8a-0329a5378324 | Benchmark_Fraisthorpe               | United_Kingdom | True   |        0 |
| 18 | 8902a43e-200e-4ef5-9299-7fa76b0d8273 | Benchmark_Pioneer_Prairie_I-II      | United_States  | True   |        7 |
| 19 | c3c6e81b-03a0-46fb-a20c-bf1078f63d64 | Benchmark_Shin-Izumo                | Japan          | True   |        0 |
| 20 | 200247e5-309a-4191-ac4e-1c58cf72aa1e | Benchmark_Pestera                   | Romania        | True   |        0 |
| 21 | fe750d15-1d49-40be-abc8-49800cd8ce49 | Benchmark_Eolien_Catalan            | France         | True   |        9 |
| 22 | c4036bb9-b4d3-449f-91b9-86c3be1baa6c | Benchmark_Amasya                    | Turkey         | True   |        0 |
| 23 | fc44e236-15df-4847-9e11-ae8b1fea04b1 | Benchmark_Corbera_y_Vilalba         | Spain          | True   |       21 |
| 24 | e4b4cf9d-a6b5-4aea-9d7f-c14f6bd0b5f5 | Benchmark_Linowo                    | Poland         | True   |        0 |
| 25 | 9f2dfc89-6419-47c6-b3a8-90f818e4af88 | Benchmark_Amliden                   | Sweden         | False  |      NaN |
| 26 | 9fe9b703-0a40-40d7-9ae8-e8c8dc5976d1 | Benchmark_Coonooer_Bridge           | Australia      | True   |        0 |
| 27 | 4e2a2113-c370-4483-a0ac-e93da6baac03 | Benchmark_Three_Mile_Canyon         | United_States  | True   |        0 |
| 28 | 75a5a608-3183-4465-9839-5723aa058640 | Benchmark_Timber_Road_II            | United_States  | True   |      158 |
| 29 | f90abf10-f7a2-47fc-843e-f00df33c1b60 | Benchmark_Bothe                     | India          | True   |       24 |
| 30 | e28566f6-484e-4f3f-99bb-42b1e68ae411 | Benchmark_TERNA-Agios_Georgios      | Greece         | True   |        9 |
| 31 | 081c3621-3acd-48b2-8f86-4661cb927521 | Benchmark_El_Porvenir               | Mexico         | True   |        0 |
| 32 | 986b30f3-f949-4b07-98b4-768dbf614a8e | Benchmark_Roosevelt                 | United_States  | False  |      NaN |
| 33 | 4c02b49c-6c92-4d43-bab1-adbc1df503ee | Benchmark_Little_Elk                | United_States  | True   |       93 |
| 34 | 5388642a-bd92-4a00-a4d3-cf10b9885a70 | Benchmark_Ostaszewo                 | Poland         | True   |        0 |
| 35 | d7e39c84-17d7-4059-a09b-3d240eef5e6b | Benchmark_Bandirma                  | Turkey         | True   |        0 |
| 36 | 2d2d6ed4-41c2-4889-9272-7f1bb4f6c27d | Benchmark_Ogorje                    | Croatia        | True   |        0 |
| 37 | f48d46b5-c2e6-4a1f-8e06-b75401f70dfe | Benchmark_Bingham                   | United_States  | True   |        0 |
| 38 | a32abb90-d32e-4062-8982-71bf0f5c7dc8 | Benchmark__Beregovaya_and_Stavki_   | Ukraine        | True   |        0 |
| 39 | e8ccff81-402d-46fa-aef7-45ee58d5580a | Benchmark_Brotorp_                  | Sweden         | True   |        9 |
| 40 | 18301f15-d509-4f6d-8c41-45f65111df61 | Benchmark_Suloglu                   | Turkey         | True   |        0 |
| 41 | a205d780-be34-4a7f-909c-a28bdfbf6f04 | Benchmark_Macho_Springs             | United_States  | True   |        0 |
| 42 | 1c9c7e25-f47a-4e29-bef3-19da8406db63 | Benchmark_Magic_Valley              | United_States  | False  |      NaN |
| 43 | d5ce0867-e85b-4145-ba98-76da98226584 | Benchmark_Gebeleisis                | Romania        | True   |        7 |
| 44 | 61356664-a203-4652-80f4-3693ffe6982f | Benchmark__Botievo_                 | Ukraine        | True   |       30 |
| 45 | 41f4ee37-3c46-4cc6-99c0-33439297ccfa | Benchmark_Musselroe                 | Australia      | True   |        0 |
| 46 | 0fdeb110-ddab-41ca-ae4c-30e786516c9a | Benchmark_Route_66                  | United_States  | True   |        0 |
| 47 | 59c3e219-0f72-4573-bf49-9bcf2831eb1f | Benchmark_Lake_Bonney_Stage_2_and_3 | Australia      | True   |       69 |
| 48 | 1fa818fa-1738-43d6-951f-4864fc9d3d94 | Benchmark_Korytnica                 | Poland         | True   |        0 |
| 49 | 780be8d2-504a-4858-9694-b8665ae98983 | Benchmark_Woolnorth_Studland_Bay    | Australia      | True   |       37 |
| 50 | 2e42ec88-bb33-4cf1-8730-f8bc61b23337 | Benchmark_Glenay                    | France         | True   |       15 |
| 51 | 7c55d3f6-f9d2-42a2-8fa6-605d50b39d61 | Benchmark_Pleasant_Valley           | United_States  | True   |        0 |
| 52 | 4a6077f7-4ce0-4ac6-af05-13afe46f88ae | Benchmark_Samdal                    | South_Korea    | True   |        0 |
| 53 | 306383c2-3cd6-4cfb-93d4-a52fe062019e | Benchmark_Milo                      | United_States  | True   |      125 |
| 54 | 69083277-1eca-4695-92bb-0e1328b56e44 | Benchmark_Tafila                    | Jordan         | False  |      NaN |
| 55 | 25213859-d726-42b0-a20e-41803623c497 | Benchmark_Goinj_Powerica            | India          | True   |        0 |
| 56 | 8b7c7bfb-22c6-496a-9de7-2ffa4b3e8e40 | Benchmark_Los_Vientos_III           | United_States  | True   |      155 |
| 57 | a3338fea-383b-403b-bf51-99a95aa8b147 | Benchmark_Carpinone                 | Italy          | True   |       45 |
| 58 | 5f733bbe-9b06-4e13-8b73-fccc5f329dfe | Benchmark_Slate_Creek               | United_States  | True   |        0 |
| 59 | 52b93fd8-5090-4aa3-91c5-e45053cef8de | Benchmark_Fallago_Rig               | United_Kingdom | True   |        0 |
| 60 | ee7c395c-4c3c-46c9-94e8-737cd0fb158c | Benchmarker_Collgar                 | Australia      | True   |        0 |
| 61 | b4b9bdf5-c0f2-4f9f-a979-2eb697e54ce8 | Benchmark_Nojoli                    | South_Africa   | True   |      132 |
| 62 | accdfa2a-6678-4cce-8016-2cf12489cecd | Benchmark_Burgos_Este               | Spain          | True   |        0 |
| 63 | 54ca7238-510d-4767-826e-4ebf5f4f5a36 | Benchmark__Munderfing_              | Austria        | True   |        0 |
| 64 | de34dbe2-3050-4ddd-ad89-401c72718435 | Benchmark_Banie_and_Kozielice       | Poland         | True   |        0 |
| 65 | 8b0fb296-6a40-4e31-8d55-e62acc920a38 | Benchmark_Iwata                     | Japan          | True   |        1 |
| 66 | e301c52e-01df-41a7-9a8a-0fb7f28f43de | Benchmark_Phu_Lac                   | Vietnam        | True   |       20 |
| 67 | e80506c7-5350-48c3-9271-28f1ec6eaee6 | Benchmark_Vandhiya-Jangi            | India          | True   |      133 |
| 68 | ad2c99e4-b7c5-4f99-98bf-b314d3a9f2af | Benchmark_Nooriabad                 | Pakistan       | True   |      197 |
| 69 | 806c84cf-7b7b-45b7-80ce-b3225377b491 | Benchmark_Los_Vientos_V             | United_States  | True   |      200 |
| 70 | ab0b13f7-d052-43cb-a5aa-c86a47a6217d | Benchmark_Renaico_I                 | Chile          | True   |       42 |
| 71 | d73058c4-2961-49b3-8775-1850550f1abf | Benchmark_Fabodliden                | Sweden         | True   |        0 |
| 72 | bbe9c3c4-4523-4fd4-97db-1fd0e709e7f8 | Benchmark_Grassridge                | South_Africa   | True   |        0 |
| 73 | f46c225d-0442-4a4b-9c04-b5a8e35eb8e2 | Benchmark_Horse_Butte               | United_States  | True   |        0 |
| 74 | d6293c21-9025-4afe-8b13-34fbf703a066 | Benchmark_Longhorn_North            | United_States  | True   |        0 |
| 75 | 41498a35-7ba9-4c11-8e13-4be111271c93 | Benchmark_Rawson                    | Argentina      | False  |      NaN |
| 76 | aabdf0e8-d335-4e3f-8fb1-4c1eb088c892 | Benchmark_La_Croix_Benjamin         | France         | True   |       18 |
| 77 | f400b943-0159-4f2f-a36c-e30e0c4289be | Benchmark__LosBuenosAires_          | Chile          | False  |      NaN |
| 78 | 8beeeb10-0fc5-425c-a1cc-744a2c60c78a | Benchmark_Facaeni_EDPR              | Romania        | True   |        0 |
| 79 | 0410f736-9499-43aa-b974-baa1f0151621 | Benchmark_Waterloo                  | Australia      | True   |        6 |
| 80 | 74b112ae-0869-4b26-b09c-bac293443a0f | Benchmarker_Mac_Arthur              | Australia      | True   |        0 |
| 81 | 91f0a8b6-86d5-40c7-ad17-61e0dbc6771f | Benchmark_Taralga                   | Australia      | True   |        0 |
| 82 | 12b0ef60-9cc9-41c7-9ae6-61f093998c4e | Benchmark_Fowler                    | United_States  | True   |      238 |
| 83 | f81a77ca-f2b4-4f3a-bacb-cb8227810f2e | Benchmark_Mahinerangi               | New_Zealand    | True   |        0 |
| 84 | 0e7a746c-2017-4760-b474-90b20640ed92 | Benchmark_Tararua_III               | New_Zealand    | True   |      200 |
| 85 | b31fe844-a7d6-4785-8dc4-07dda9f87b03 | Benchmark_Macambira                 | Brazil         | True   |        0 |
| 86 | db5adca0-0e3e-4b7c-a088-f78d0be808c0 | Benchmark_Coulommes_et_Marqueny     | France         | False  |      NaN |
| 87 | 0a28bc48-c6c4-460f-9c46-f7a20a980caf | Benchmark_Castellaneta_EDPR         | Italy          | True   |       40 |
| 88 | 294eb348-131b-4dd2-9069-2715414b9c26 | Benchmark_Sherbino_Mesa             | United_States  | True   |        0 |
|----+--------------------------------------+-------------------------------------+----------------+--------+----------|

****** Site corners
:PROPERTIES:
:ID:       37029184-8ca4-41f0-9424-5aa20a0f7dfc
:ARCHIVE_TIME: 2019-05-07 Tue 09:23
:ARCHIVE_FILE: ~/NextCloud/org/work.org
:ARCHIVE_OLPATH: Vestas/PSE2/Benchmark
:ARCHIVE_CATEGORY: work
:ARCHIVE_TODO: DONE
:END:

- State "DONE"       from "TODO"       [2019-03-25 Mon 16:36]

****** Interpolate power and thrust curves
:PROPERTIES:
:ARCHIVE_TIME: 2019-05-07 Tue 09:23
:ARCHIVE_FILE: ~/NextCloud/org/work.org
:ARCHIVE_OLPATH: Vestas/PSE2/Benchmark
:ARCHIVE_CATEGORY: work
:ARCHIVE_TODO: DONE
:END:

- State "DONE"       from "TODO"       [2019-05-02 Thu 11:30]
- State "TODO"       from "NOT TODO"   [2019-05-01 Wed 16:12]
- State "NOT TODO"   from              [2019-04-15 Mon 11:44] \\
  Bad communication

#+begin_src python
  import pandas as pd
  import numpy as np


  def interpolate_pc(df, air_density, curve, method='pchip',
                     ws=list(np.arange(0, 30.5, 0.5))+[100]):
      """Interpolates either power or thrust curves to the desired air density, at
      the desired windspeeds. By default, the windspeeds are given by the ws
      variable.


      :param df: pd.DataFrame of the following form:

      |-------+------------+---------------------------+-----------|
      |       | AirDensity | PowerKw/ThrustCoeffecient | WindSpeed |
      |-------+------------+---------------------------+-----------|
      |     0 |      1.225 |                      81.0 |       3.0 |
      | (...) |            |                           |           |
      |    23 |        1.0 |                      64.0 |       3.0 |
      |-------+------------+---------------------------+-----------|

      :param air_density: float

      :param curve: str that speficies which curve to interpolate (either 'power'
      or 'thrust)

      :param ws: list of windspeeds where the interpolation is evaluated

      :returns: Data Frame of the following form:

      |-------+-----------+---------------------------|
      |       | WindSpeed | PowerKw/ThrustCoeffecient |
      |-------+-----------+---------------------------|
      |     0 |       0.0 |                       0.0 |
      | (...) |           |                           |
      |-------+-----------+---------------------------|

      Its length is the length of ws.

      :rtype: pd.DataFrame

      """
      if curve == 'power':
          param = 'PowerKw'

      if curve == 'thrust':
          param = 'ThrustCoeffecient'

      def _interpolate_to_air_density():
          """Intermediary step to interpolate the curve to the desired air density.

          """

          if air_density not in df['AirDensity'].unique():
              any_density = df['AirDensity'].loc[0]
              nan_block = df[df['AirDensity'] == any_density].copy()
              nan_block[param] = np.nan
              nan_block['AirDensity'] = air_density

              nans = pd.concat([df, nan_block], ignore_index=True)\
                       .sort_values(by=['AirDensity', 'WindSpeed'])

              return nans.groupby('WindSpeed')\
                         .apply(lambda group: group.set_index('AirDensity')
                                .interpolate(method='index',
                                             limit_direction='both'))\
                         .loc[(slice(None), air_density), :]\
                         .reset_index('WindSpeed', drop=True)\
                         .reset_index(drop=True)
          else:
              return df.set_index('AirDensity')\
                       .loc[air_density]\
                       .reset_index(drop=True)

      curve_at_density = _interpolate_to_air_density()

      nan_block = pd.DataFrame(
          {'WindSpeed': list(set(ws) - set(curve_at_density['WindSpeed'])),
           param: np.nan})

      nans = pd.concat([curve_at_density, nan_block], ignore_index=True)\
               .sort_values(by=['WindSpeed', param])\
               .reset_index(drop=True)

      nans[param][nans['WindSpeed'] < curve_at_density['WindSpeed'].min()] = 0
      nans[param][nans['WindSpeed'] > curve_at_density['WindSpeed'].max()] = 0

      return nans.set_index('WindSpeed')\
                 .interpolate(method=method,
                              limit_direction='both')\
                 .reset_index()


  # json_ex = 'ac_inputs_pad_constrained.json'
  # # json_ex = 'ac_inputs.json'
  # with open(json_ex) as jsonf:
  #     jdata = json.load(jsonf)['power_ct']

  # # id
  # # old '2003606'
  # # new '1004731'

  # pc_ids = ['2003606', '1004731', '2003523']
  # kinds = ['power', 'thrust']
  # json_keys = ['power_curve', 'ct_curve']

  # pc_id = pc_ids[1]
  # kind = kinds[0]
  # json_key = json_keys[0]
  # curve_df = pd.DataFrame.from_dict(jdata[pc_id][json_key])
  # densities = curve_df['AirDensity'].unique()
  # density = float(densities[3]) + 0.01

  # title = 'MacArthur new Thrust'
  # ax = interpolate_pc(curve_df, density, kind).iloc[:-1]\
  #                                             .plot(kind='scatter',
  #                                                   x='WindSpeed',
  #                                                   y='PowerKw',
  #                                                   title=title)
  # fig = ax.get_figure()
  # fig.savefig(title)
  # print(interpolate_pc(curve_df, density, kind))
#+end_src

****** Refactor json
SCHEDULED: <2019-05-21 Tue>
:PROPERTIES:
:ARCHIVE_TIME: 2019-05-07 Tue 09:23
:ARCHIVE_FILE: ~/NextCloud/org/work.org
:ARCHIVE_OLPATH: Vestas/PSE2/Benchmark
:ARCHIVE_CATEGORY: work
:ARCHIVE_TODO: DONE
:header-args:python: :tangle /ssh:aadco@login.mindstorm.vestas.net:~/refactor_json.py
:header-args:sh: :dir /ssh:aadco@login.mindstorm.vestas.net:~/
:END:

- State "DONE"       from "TODO"       [2019-05-21 Tue 11:26]
- State "TODO"       from "DONE"       [2019-05-20 Mon 09:55]
- State "DONE"       from "TODO"       [2019-05-03 Fri 09:55]

#+begin_src python
  import copy
  import json
  import os
  import pandas as pd
  import math as m


  def super_hero_pc(path):
      df = pd.read_csv(path)
      df = pd.melt(df, id_vars=['ws'])
      df.rename(columns={'ws': 'WindSpeed',
                         'variable': 'AirDensity',
                         'value': 'PowerKw'}, inplace=True)
      df['AirDensity'] = df['AirDensity'].astype('float')
      return df.to_dict('records')


  def super_hero_ct(path):
      df = pd.read_csv(path)
      df = pd.melt(df, id_vars=['ws'])
      df.rename(columns={'ws': 'WindSpeed',
                         'variable': 'AirDensity',
                         'value': 'ThrustCoeffecient'}, inplace=True)
      df['AirDensity'] = df['AirDensity'].astype('float')
      return df.to_dict('records')


  def transf_to_pad_constrained(root,
                              json_initial_fn='ac_inputs.json',
                              json_final_fn='ac_inputs_pad_constrained.json'):

      with open(os.path.join(root, json_initial_fn), 'r') as json_file_initial:
          d = json.load(json_file_initial)

      meta = d['Meta']
      meta['max_capacity'] = None
      ext_turb = [t for t in meta['ud_layout']
                  if t['pad_type'] == 'external']
      if ext_turb == []:
          ext_turb = None
          d['pfmp_ext'] = None
      turb = [t for t in meta['ud_layout']
              if t['pad_type'] != 'external']
      super_hero_turb = copy.deepcopy(turb)
      [t.update({'wtg_id': '1004731'}) for t in super_hero_turb]
      meta['layout'] = {'0': turb,
                        '1': super_hero_turb}
      meta['external_layout'] = ext_turb
      del meta['ud_layout']
      del meta['n_case']
      del meta['n_config']
      pfmp = d['pfmp']
      if ext_turb != []:
          d['pfmp_ext'] = pfmp['0']
      pfmp['1'] = {'1004731':
                   {'BaseVariantIdentifier': 1004731,
                    'ShortName': 'FSCS',
                    'Supplier': 'Vestas',
                    'RotorDiameter': 150.0,
                    'HubHeight': 145.0,
                    'NominalPowerKw': 4000,
                    'MkVersion': 'MK3E',
                    'Converter': 'Full Scale Converter System',
                    'PowerCurveIdentifier': 1004731,
                    'Frequency': '50/60Hz',
                    'WindTurbineClass': 'N/A',
                    'MainPlatform': '3MW',
                    'air_density': pfmp['0'][list(pfmp['0'].keys())[0]]['air_density'],
                    'Status': 'Restricted',
                    'ChangeBy': 'VESTAS\\hefar',
                    'ChangeDate': '2017-08-23T06:34:00',
                    'Valid': True,
                    'PhasedOut': False,
                    'HasDefaultRunMode': True,
                    'n_wtg': len(turb),
                    'LoadModelFilename': '/ifs/dm/cfd/app/PSE2/TurbineXMLdecrypted/150_4000_IEC3B_3E_5060_GS_STD-T966906-TowerModel_105.0.xml'}}
      power_ct = d['power_ct']
      power_ct['1004731'] = {'ct_curve': super_hero_ct('/ifs/home/aadco/ct.csv'),
                             'power_curve': super_hero_pc('/ifs/home/aadco/pc.csv')}

      with open(os.path.join(root, json_final_fn), 'w') as json_file_final:
          json.dump(d, json_file_final)


  def transf_to_mw_constrained(root,
                             json_initial_fn='ac_inputs.json',
                             json_final_fn='ac_inputs_mw_constrained.json'):

      with open(os.path.join(root, json_initial_fn), 'r') as json_file_initial:
          d = json.load(json_file_initial)
      meta = d['Meta']
      meta['max_pad'] = None
      meta['layout'] = None
      ext_turb = [t for t in meta['ud_layout']
                  if t['pad_type'] == 'external']
      if ext_turb == []:
          ext_turb = None
          d['pfmp_ext'] = None
      meta['external_layout'] = ext_turb
      del meta['ud_layout']
      del meta['n_case']
      del meta['n_config']
      pfmp = d['pfmp']
      turbine_ids = pfmp['0'].keys()
      [pfmp['0'][turb_id].pop('n_wtg')
       for turb_id in turbine_ids]
      if ext_turb != []:
          d['pfmp_ext'] = pfmp['0']
      pfmp['1'] = {'1004731':
                   {'BaseVariantIdentifier': 1004731,
                    'ShortName': 'FSCS',
                    'Supplier': 'Vestas',
                    'RotorDiameter': 150.0,
                    'HubHeight': 145.0,
                    'NominalPowerKw': 4000,
                    'MkVersion': 'MK3E',
                    'Converter': 'Full Scale Converter System',
                    'PowerCurveIdentifier': 1004731,
                    'Frequency': '50/60Hz',
                    'WindTurbineClass': 'N/A',
                    'MainPlatform': '3MW',
                    'air_density': pfmp['0'][list(pfmp['0'].keys())[0]]['air_density'],
                    'Status': 'Restricted',
                    'ChangeBy': 'VESTAS\\hefar',
                    'ChangeDate': '2017-08-23T06:34:00',
                    'Valid': True,
                    'PhasedOut': False,
                    'HasDefaultRunMode': True,
                    'LoadModelFilename': '/ifs/dm/cfd/app/PSE2/TurbineXMLdecrypted/150_4000_IEC3B_3E_5060_GS_STD-T966906-TowerModel_105.0.xml'}}
      power_ct = d['power_ct']
      power_ct['1004731'] = {'ct_curve': super_hero_ct('/ifs/home/aadco/ct.csv'),
                             'power_curve': super_hero_pc('/ifs/home/aadco/pc.csv')}

      with open(os.path.join(root, json_final_fn), 'w') as json_file_final:
          json.dump(d, json_file_final)


  def shift_latlon(lat, lon, km):
      """
      Latitude: 1 deg =  110.574 km
      Longitude: 1 deg = 111.320*cos(latitude) km

      :param coord_list:
      :param km:
      :returns:
      :rtype:

      """
      lon += km/(111.320 * m.cos(m.radians(lat)))
      lat += km/110.574
      return {'Latitude': lat, 'Longitude': lon}


  def shift_corners(root,
                  json_initial_fn='vsu_inputs.json',
                  json_final_fn='vsu_inputs_new_corners.json'):
      with open(os.path.join(root, json_initial_fn), 'r') as json_file_initial:
          d = json.load(json_file_initial)

      corners = d['Meta']['site_corners']
      new_corners = [shift_latlon(corner['Latitude'],
                                  corner['Longitude'],
                                  (-1)**(i+1) * 0.5)
                     for i, corner in enumerate(corners[:2])]
      new_corners.append({'Latitude': new_corners[0]['Latitude'],
                          'Longitude': new_corners[1]['Longitude']})
      new_corners.append({'Latitude': new_corners[1]['Latitude'],
                          'Longitude': new_corners[0]['Longitude']})
      d['Meta']['site_corners'] = new_corners

      with open(os.path.join(root, json_final_fn), 'w') as json_file_final:
          json.dump(d, json_file_final)


  path = '/ifs/dm/cfd/app/PSE2/benchmark_pad_constrained'
  for proj in os.listdir(path):
      root = os.path.join(path, proj)
      os.rename(os.path.join(root, "vsu_inputs.json"),
                os.path.join(root, "vsu_inputs_old.json"))
      os.rename(os.path.join(root, "vsu_inputs_new_corners.json"),
                os.path.join(root, "vsu_inputs.json"))
      # shift_corners(root)
      # transf_to_pad_constrained(root)


  path = '/ifs/dm/cfd/app/PSE2/benchmark_mw_constrained'
  for proj in os.listdir(path):
      root = os.path.join(path, proj)
      os.rename(os.path.join(root, "vsu_inputs.json"),
                os.path.join(root, "vsu_inputs_old.json"))
      os.rename(os.path.join(root, "vsu_inputs_new_corners.json"),
                os.path.join(root, "vsu_inputs.json"))
      # shift_corners(root)
      # transf_to_mw_constrained(root)
#+end_src

#+begin_src sh
  module load anaconda/2-5.1.0
  python refactor_json.py &
#+end_src

#+RESULTS:
***** Testing
#+begin_src python
  import pandas as pd
  import numpy as np
  import pandas.util.testing as tm

  #####################
  ### preliminaries ###
  #####################

  # common path where data is stored, read and written
  common_path = '/ifs/dm/cfd/app/PSE2/pytestInputs/meas_test_data/'

  # mesoscale data from Porto limited to 10k lines (no need to run the tests with the complete dataset)
  data = pd.read_csv(common_path + 'meso_porto_raw.csv').iloc[0:10000]

  # selecting and renaming the needed columns from the data
  # the last two temperature columns are being converted to Celcius degrees
  ts = data.filter(like='TimeStamp', axis=1)
  ws160 = data.filter(regex='WSP160', axis=1).rename(
      columns={'WSP160': '- WindSpeed 160 Mean - -'})
  ws140 = data.filter(regex='WSP140', axis=1).rename(
      columns={'WSP140': '- WindSpeed 140 Mean - -'})
  wd160 = data.filter(regex='WDIR160', axis=1).rename(
      columns={'WDIR160': '- WindDirection 160 Mean - -'})
  wd140 = data.filter(regex='WDIR140', axis=1).rename(
      columns={'WDIR140': '- WindDirection 140 Mean - -'})
  rh160 = data.filter(regex='RH160', axis=1).rename(
      columns={'RH160': '- RelativeHumidity 160 Mean - -'})
  rh140 = data.filter(regex='RH140', axis=1).rename(
      columns={'RH160': '- RelativeHumidity 140 Mean - -'})
  tk160 = data.filter(regex='TK160', axis=1).rename(
      columns={'TK160': '- Temperature 160 Mean - -'}) - 273.15
  tk140 = data.filter(regex='TK140', axis=1).rename(
      columns={'TK160': '- Temperature 140 Mean - -'}) - 273.15

  # setting up raw testing data for data quality check
  pd.concat([ts, ws160, wd160, rh160], axis=1).to_csv(
      common_path + 'dqc/dqc_raw.csv', index=False)

  # setting up raw testing data for icing
  # creating a synthethic wind speed std deviation column since this data isn't provided by the mesoscale
  ws_std160 = ws160*0.07
  ws_std160.columns = ['- WindSpeed 160 StdDev - -']
  pd.concat([ts, ws160, ws_std160, wd160, tk160], axis=1).to_csv(
      common_path + 'icing/icing_raw.csv', index=False)

  # setting up raw testing data for tower shading
  pd.concat([ts, ws160, ws140, wd160, wd140], axis=1).to_csv(
      common_path + 'towershading/towershading_raw.csv', index=False)

  ##########################
  ### data_quality_check ###
  ##########################

  dqc_common_path = common_path + 'dqc/dqc_'


  def pollute_dqc(n, polluted_cols, ratio):
      """
      Writes the *polluted.csv and *nan.csv datasets for test_data_quality_checks.


      Inputs:
      n (int) : integer associated with a specific test

      polluted_cols (list of strings) : list containing regex strings that specify which columns of the data will be polluted with non-physical data (for instance, negative wind speed)

      ratio (list of floats) : ratio of polluted data per column (1 means no pollution; 0 means totally polluted)
      """
      global dqc_common_path
      data = pd.read_csv(dqc_common_path + 'raw.csv')
      n_cols = len(ratio)
      # list s.t. each element is a list of missing indices of the data (ratio). those indices are the choosen ones to be polluted
      index = [tm._create_missing_idx(
          nrows=data.shape[0], ncols=1, density=ratio[i])[0] for i in range(n_cols)]
      # list of columns to be polluted
      p_cols = [data.filter(regex=polluted_cols[i],
                            axis=1).columns for i in range(n_cols)]
      nandata = data.copy()
      for i in range(n_cols):
          # inserting nans in the right places
          nandata.loc[index[i], p_cols[i]] = np.nan
          # inserting non physical windspeed
          if i == 0:
              data.loc[index[i], p_cols[i]
                       ] = np.random.uniform(-100, 0) or np.random.uniform(100, 200)
          # inserting non physical wind directions
          if i == 1:
              data.loc[index[i], p_cols[i]
                       ] = np.random.uniform(-360, 0) or np.random.uniform(361, 920)
          # inserting non physical relative humidity
          if i == 2:
              data.loc[index[i], p_cols[i]
                       ] = np.random.uniform(-100, 0)
      # writing the files
      data.to_csv(dqc_common_path + 'polluted' + str(n) + '.csv', index=False)
      nandata.to_csv(dqc_common_path + 'nan' + str(n) +
                     '.csv', na_rep=np.nan, index=False)


  # calling the above function to generate the needed datasets for test_data_quality_checks
  pollute_dqc(0, ['WindSpeed', 'WindDirection', 'RelativeHumidity'], [1]*3)

  pollute_dqc(1, ['WindSpeed', 'WindDirection', 'RelativeHumidity'], [0]*3)

  pollute_dqc(2, ['WindSpeed', 'WindDirection',
                  'RelativeHumidity'], [np.random.rand()]*3)


  ####################
  ### filter_icing ###
  ####################

  icing_common_path = common_path + 'icing/icing_'


  def block():
      """This function gets raw data from the mesocale model and outputs blocks of data. The output of this function is then used by the test_icing_generator function to generate datasets that test the icing filter.

      If the 2 conditions below hold true for periods of time >= 5 hours, then that's regarded as icing:
      A : WS_standard_deviation < 0.35
      B : Temperature < 3 ºC

      | Block  | Condition A | Condition B | Time >= 5 |
      |--------+-------------+-------------+-----------|
      | block0 | F           | F           | T         |
      | block1 | T           | F           | T         |
      | block2 | F           | T           | T         |
      | block3 | T           | T           | F         |
      | block4 | T           | T           | T         |

      Only block4 would be regarded as icing.
      When an icing event is found the wind speed mean, wind speed std deviation and wind direction mean are discarded. The nanblock mimics the output of passing block4 as input for the icing filter, i.e. inserting nans in the referred variables."""
      global icing_common_path
      # generating the lengths for each block except block3: 30 [5 hours] <= random number < 1500 [250 hours]
      block_length = np.random.uniform(30, 1500, 4).astype("int")
      # appending the length for block3: 1 [10 min] <= random number < 29 [4 hours 50 min]
      block_length = np.append([0, int(np.random.uniform(1, 29))], block_length)
      # list containing the start and end indices for each block.
      idx = [np.sum(block_length[:i]) for i in range(1, len(block_length)+1)]
      # loading the data up to the necessary index, i.e. till the last entry of the index list
      data = pd.read_csv(icing_common_path + 'raw.csv').iloc[:idx[-1]]
      # generating random numbers that match conditions A and B (either if they hold true [1] or false [0]).
      WS_STD_0 = np.random.uniform(0.36, 1)
      WS_STD_1 = np.random.uniform(0, 0.35)
      T_0 = np.random.uniform(3.1, 30)
      T_1 = np.random.uniform(-20, 3.1)
      # populating the blocks
      # notice that the first two entries of the index list correspend to block3.
      data.iloc[idx[1]:idx[2], 2] = WS_STD_0
      data.iloc[idx[1]:idx[2], 4] = T_0
      block0 = data.iloc[idx[1]:idx[2], :]
      ###
      data.iloc[idx[2]:idx[3], 2] = WS_STD_1
      data.iloc[idx[2]:idx[3], 4] = T_0
      block1 = data.iloc[idx[2]:idx[3], :]
      ###
      data.iloc[idx[3]:idx[4], 2] = WS_STD_0
      data.iloc[idx[3]:idx[4], 4] = T_1
      block2 = data.iloc[idx[3]:idx[4], :]
      ###
      data.iloc[idx[0]:idx[1], 2] = WS_STD_1
      data.iloc[idx[0]:idx[1], 4] = T_1
      block3 = data.iloc[idx[0]:idx[1], :]
      ###
      data.iloc[idx[4]:idx[5], 2] = WS_STD_1
      data.iloc[idx[4]:idx[5], 4] = T_1
      block4 = data.iloc[idx[4]:idx[5], :]
      ###
      nanblock = block4.copy()
      nanblock.iloc[:, 1:4] = np.nan
      return block0, block1, block2, block3, block4, nanblock


  def test_icing_generator(n, permutation):
      """Creates the datasets (both *polluted.csv and *nan.csv) to be used by test_filter_icing. To achieve this, it concatenates and permutates the blocks of the block() function.

      Inputs:
      n (int) : test number
      permutation (list of integers) : dictates the order of the blocks

      In order to have control over the dataset (predicting the output of the icing filter, i.e. anticipate where the nans will be inserted), block3 and block4 shouldn't be consecutive. Otherwise, both would be regarded as icing and the block() method would only regard block4 as icing.

  ||||    What follows is a simple combinatorial reasoning.
  ||||    Let's see which block configurations suit our needs.
  ||||    Excluding any constraints, the 5 blocks can generate 5! = 120 datasets. However, recalling the above mentioned constraint, the allowed configurations are 5! - (4 * 2! * 3!) = 72.
  ||||    The 4 prohibited configurations are exemplified below (the integers represent the blocks associated with that integer, and the '.' represents any of the remaining blocks):
  ||||
  ||||    [3 4 . . .]
  ||||
  ||||    [. 3 4 . .]
  ||||
  ||||    [. . 3 4 .]
  ||||
  ||||    [. . . 3 4]
  ||||
  ||||
  ||||    The allowed configurations are far too many and most of them are redundant in this context. Let's look at it through another angle.
  ||||    Our constraint is concerned with choosing non consecutive positions for blocks 3 and 4, so let's fix their positions and disregard any permutations.
  ||||    The table below presents the different configurations up to permutations:
  ||||    ("up to permutations" is a precise mathematical jargon. It means that, for instance, [3 0 4 1 2], [4 0 3 1 2], [3 0 4 2 1] and [3 1 4 0 2] belong to the same configuration.)
  ||||
  ||||    | Configuration | # Permutations |
  ||||    |---------------+----------------|
  ||||    | [3 0 4 1 2]   | 2! * 3!        |
  ||||    | [0 3 1 4 2]   | 2! * 3!        |
  ||||    | [1 0 3 2 4]   | 2! * 3!        |
  ||||    | [3 0 1 4 2]   | 2! * 3!        |
  ||||    | [0 3 2 1 4]   | 2! * 3!        |
  ||||    | [3 0 2 1 4]   | 2! * 3!        |
  ||||
  ||||
  ||||    Notice that the position of blocks 3 and 4 is fixed. For each configuration type, permutating blocks 3 and 4 aaccounts for 2! and permutating blocks 0, 1 and 2 accouts for 3!.
  ||||    Notice that 6 * (2! * 3!) = 72, as expected.
  ||||    Notice that (5! - (4 * 2! * 3!) ) / (3! * 2!) = 6, also as expected.
  ||||    For our purposes, generating a test for each of the 6 configurations should suffice.
      """
      global icing_common_path
      # getting the blocks
      b = block()
      # getting the index of block4
      special_index = permutation.index(4)
      # creating a copy of the permutation list and add one unit at the special_index since block()[4] corresponds to block4 and block()[4+1] corresponds to the nanblock.
      permutation_nan = permutation.copy()
      permutation_nan[special_index] = permutation[special_index] + 1
      # concatenating the blocks and generating *polluted.csv files and *nan.csv files
      data = pd.concat([b[permutation[0]], b[permutation[1]],
                        b[permutation[2]], b[permutation[3]], b[permutation[4]]])
      nandata = pd.concat([b[permutation_nan[0]], b[permutation_nan[1]],
                           b[permutation_nan[2]], b[permutation_nan[3]], b[permutation_nan[4]]])
      data.to_csv(icing_common_path + 'polluted' + str(n) + '.csv', index=False)
      nandata.to_csv(icing_common_path + 'nan' + str(n) +
                     '.csv', na_rep=np.nan, index=False)


  # generating the files for test 0 (no icing) of test_filter_icing.
  no_icing = block()[0]
  no_icing.to_csv(icing_common_path + 'polluted0.csv', index=False)
  no_icing.to_csv(icing_common_path + 'nan0.csv', index=False)

  # generating the files for test 1 (full icing) of test_filter_icing.
  full_icing = block()
  full_icing[4].to_csv(icing_common_path + 'polluted1.csv', index=False)
  full_icing[5].to_csv(icing_common_path + 'nan1.csv', index=False)

  # generating the files for test[2:] of test_filter_icing.
  test_icing_generator(2, [3, 0, 4, 1, 2])
  test_icing_generator(3, [0, 3, 1, 4, 2])
  test_icing_generator(4, [1, 0, 3, 2, 4])
  test_icing_generator(5, [3, 0, 1, 4, 2])
  test_icing_generator(6, [0, 3, 2, 1, 4])
  test_icing_generator(7, [3, 0, 2, 1, 4])

  #####################
  ### tower_shading ###
  #####################

  ts_path = common_path + 'towershading/towershading_'

  # creating the first dataset for test0 (raw mesoscale data)
  # at first, it would clear that no tower shading should be expected but ...(to be continued...)
  data = pd.read_csv(ts_path + 'raw.csv')

  data.to_csv(ts_path + 'polluted0.csv', index=False)
  data.to_csv(ts_path + 'nan0.csv', index=False)

  # creating test1
  WD160 = data.values[:, 3]
  WD140 = data.values[:, 4]
  WS140 = data.values[:, 2]

  # choosing a WD, here [110,120], and setting WS = 4 m/s at a lower height.
  cond = (WD140 >= 110) & (WD140 <= 120) & (WS140 >= 4)
  index = np.where(cond)[0]
  data.iloc[index, 2] = 4
  data.to_csv(ts_path + 'polluted1.csv', index=False)

  # setting the nans in the right places
  nandata = data.copy()
  nanindex = np.where((WD160 >= 115 - 15) &
                      (WD160 <= 115 + 15))[0]
  nandata.iloc[nanindex, 2] = np.nan
  nandata.to_csv(ts_path + 'nan1.csv',
                 na_rep=np.nan, index=False)

#+end_src

****** Flow
#+begin_src python
  import pytest
  import pandas as pd
  import numpy as np

  from pse2.flow import Flow, evaluate_wasp
  from pse2.climate import Climate, get_climates_name_list


  # Overall testing philosophy: Compare the wasp computations against
  # climate data.


  def aux(hdf5filename, roughness_grd_file, elevation_grd_file):
      """
      Returns dataframes featuring the wind rose predicted by wasp and
      from climates. This is an auxiliar method that will be used to
      evaluate wasp's performance to generalize climates.

      A climate can be composed by measurements only, or long time
      corrected with mesoscale data.

      (1) By default, the Flow class takes the largest height available as
      input to wasp.

      Evaluating wasp means generalizing an input_climate in order to
      predict the wind rose at an output_climate (met mast location) and
      compare it against climate data.

      Each input_climate or output_climate corresponds to (x,y)
      coordinates of met_masts. Notice that, for a given site, the set of
      input_climates coincides with the set of output_climates. Let C be
      this set and let n be its cardinality, i.e. the number of climates.

      C = { (x_i,y_i) such that i belongs to {1,...,n} }
      |C| = n

      Notice that the height was not taken into account in the reasoning
      above, otherwise the set of input_climates and output_climates would
      differ. Recall that the height of the input_climates is constant
      (read (1)). Regarding the output_climates, one does not know
      beforehand how many heights are available. It is resonable to take
      the 2 largest available. Let these be referred to as top and bot.

      The table below sums up the above. Let i belong to {1,...,n}.

      | input_climates    | output_climates   |
      |-------------------+-------------------|
      | (x_i, y_i, top_i) | (x_i, y_i, top_i) |
      |                   | (x_i, y_i, bot_i) |

      Generating all possible combinations of input/output_climate is
      straightforward. Mathematically, it is C x C x H, where H = {'top',
      'bot'} and x denotes de cartesian product.

      There are 2 n^2 possible combinations.

      """
      climate_list = get_climates_name_list(hdf5filename)
      N_climates = len(climate_list)

      # The climate's name is irrelevant. Map an integer to each.
      clim_index = [i for i in range(N_climates)]
      h_index = ["top", "bot"]

      # Setting up the structre of the dataframe to accommodate all possible
      # combinations.
      row_indexing = pd.MultiIndex.from_product(
          [clim_index, clim_index, h_index],
          names=("input_climate", "evaluation_climate", "height"),
      )
      locs = pd.DataFrame(index=row_indexing, columns=["x", "y", "h", "wind_resources"])

      # Tool to access the df index effortlessly
      idx = pd.IndexSlice

      # Populating the df
      for i, climate_name in enumerate(climate_list):
          climate_obj = Climate(hdf5filename, climate_name)
          x = climate_obj.loc_x
          y = climate_obj.loc_y
          h_top = climate_obj.H[-1]
          h_bot = climate_obj.H[-2]
          A_top = climate_obj.A[-1]
          A_bot = climate_obj.A[-2]
          k_top = climate_obj.k[-1]
          k_bot = climate_obj.k[-2]
          f_top = climate_obj.f[-1]
          f_bot = climate_obj.f[-2]
          wind_resources_top = np.concatenate((A_top, k_top, f_top))
          wind_resources_bot = np.concatenate((A_bot, k_bot, f_bot))
          locs.loc[idx[:, i, "top"], 0:3] = np.tile([x, y, h_top], (N_climates, 1))
          locs.loc[idx[:, i, "bot"], 0:3] = np.tile([x, y, h_bot], (N_climates, 1))
          locs.loc[idx[:, i, "bot"], "wind_resources"] = locs.loc[
              idx[:, i, "bot"], "wind_resources"
          ].apply(lambda x: wind_resources_bot)
          locs.loc[idx[:, i, "top"], "wind_resources"] = locs.loc[
              idx[:, i, "top"], "wind_resources"
          ].apply(lambda x: wind_resources_top)

      # Running wasp at all possible combinations
      locs_array = locs.reset_index()[["x", "y", "h", "input_climate"]].values
      wasp_output = evaluate_wasp(
          locs_array, hdf5filename, roughness_grd_file, elevation_grd_file
      )[:, 0:-2]
      wasp_output_df = pd.Series(wasp_output.tolist(), index=row_indexing)

      return locs["wind_resources"], wasp_output_df


  # Passing the inputs. This pipeline is ideal since the above function should
  # only be run once per site for efficiency reasons. Extending this to more sites
  # should be straightforward - decorators, *args, **kwargs.


  fn = "/ifs/dm/cfd/app/PSE2/testcaseMacArthur/"
  hdf5filename = fn + "MacArthur_PSE2_withTurbine.h5"
  roughness_grd_file = fn + "siteInfo/roughnessMap.grd"
  elevation_grd_file = fn + "siteInfo/SRTM_UTM.grd"


  flow_data = aux(hdf5filename, roughness_grd_file, elevation_grd_file)
  locs = flow_data[0]
  wasp_output_df = flow_data[1]


  def test_metmast_self_prediction():
      """
      Evaluating the wind rose at mast location given itself as input for
      wasp.

      This is a subset of C x C x H, i.e. it is a special case of all
      possible combinations. Let's call this subset Self.

      Let i,j belong to {1,...,n}.
      Self = { (climate_i, climate_j, top) such that i = j }
      |Self| = n

      Returns true if the relative entry wise error is less than rtol.

      """
      global locs, wasp_output_df
      query_top = 'height == "top" and input_climate == evaluation_climate'
      climate_output = np.vstack(locs.to_frame().query(query_top).values.ravel())
      wasp_output = np.stack(wasp_output_df.to_frame().query(query_top).values.ravel())
      print("max : ", np.max(abs(climate_output - wasp_output) / climate_output))
      print("min : ", np.min(abs(climate_output - wasp_output) / climate_output))
      print("mean : ", np.mean(abs(climate_output - wasp_output) / climate_output))
      print("sigma : ", np.std(abs(climate_output - wasp_output) / climate_output))
      assert np.allclose(wasp_output, climate_output, rtol=0.01)


  def test_metmast_self_prediction_dif_height():
      """
      Evaluating the wind rose at mast location given itself as input for
      wasp, but at a lower height.

      This is a subset of C x C x H, i.e. it is a special case of all
      possible combinations. Let's call this subset Self_bot.

      Let i,j belong to {1,...,n}.
      Self_bot = { (climate_i, climate_j, bot) such that i = j }
      |Self_bot| = n

      Returns true if the relative entry wise error is less than rtol.

      """
      global locs, wasp_output_df
      query_bot = 'height == "bot" and input_climate == evaluation_climate'
      climate_output = np.vstack(locs.to_frame().query(query_bot).values.ravel())
      wasp_output = np.stack(wasp_output_df.to_frame().query(query_bot).values.ravel())
      print("max : ", np.max(abs(climate_output - wasp_output) / climate_output))
      print("min : ", np.min(abs(climate_output - wasp_output) / climate_output))
      print("mean : ", np.mean(abs(climate_output - wasp_output) / climate_output))
      print("sigma : ", np.std(abs(climate_output - wasp_output) / climate_output))
      assert np.allclose(wasp_output, climate_output, rtol=0.1)


  def test_mast_to_mast_prediction():
      """
      Evaluating the wind rose at all possible combinations.

      Returns true if the relative entry wise error is less than rtol.
      """
      global locs, wasp_output_df
      climate_output = np.vstack(locs.values)
      wasp_output = np.stack(wasp_output_df.values)
      print("max : ", np.max(abs(climate_output - wasp_output) / climate_output))
      print("min : ", np.min(abs(climate_output - wasp_output) / climate_output))
      print("mean : ", np.mean(abs(climate_output - wasp_output) / climate_output))
      print("sigma : ", np.std(abs(climate_output - wasp_output) / climate_output))
      assert np.allclose(wasp_output, climate_output, rtol=0.2)


  def test_flatsite():
      """
      As simple as running the above script with an hdf5filename,
      elevation_grd_file and roughness_grd_file to match a flat site.
      """
      return True


  def test_lagoa_do_barro():
      """
      As simple as giving the above script with an hdf5filename,
      elevation_grd_file and roughness_grd_file to match Lagoa do Barro.

      Interesting case since it has 25 met masts.
      """
      return True
#+end_src

****** Meas
#+begin_src python
  import pytest
  import numpy as np
  import pandas as pd

  from pse2.meas import Meas


  # Overall testing philosophy: Anticipate the output of a method and compare it
  # against its (actual) output.  The following diagram illustrates it.

  #      input --> METHOD |--> output
  #                       |     |T|
  #                       |     |E| TEST asserts if output == antecipated output.
  #                       |     |S| If True, TEST passes.
  #                       |     |T| Otherwise, TEST fails.
  #                       |--> anticipated output

  # To perform a test two things are required: an input and an anticipated output.
  #     - input: a dataframe - *polluted.csv files.
  #     - anticipated output: a dataframe - *nan.csv files.

  # The structure of the tests is always the same. Generating the dataframes is
  # the key task.  For a detailed description on how those dataframes were
  # generated have a look at
  # tests/createSyntheticMeasurementstests/auxiliar_test.py.

  common_path = "/ifs/dm/cfd/app/PSE2/pytestInputs/meas_test_data/"
  measure = Meas("", "")


  @pytest.mark.parametrize("n", [0, 1, 2])
  def test_data_quality_checks(n):
      """
      test0 : data without non physical quantities
      test1 : data 100% filled with non physical quantities
      test2 : data filled with a random ratio of non physical quantities
      """
      global common_path
      path = common_path + "dqc/dqc_"
      global measure
      measure.import_file_offline(path + "polluted" + str(n) + ".csv")
      meas_df = measure.data_quality_checks()
      anticipated_df = pd.read_csv(path + "nan" + str(n) + ".csv").drop(
          ["TimeStamp"], axis=1
      )
      assert np.allclose(meas_df.values, anticipated_df.values, equal_nan=True)


  @pytest.mark.parametrize("n", [0, 1, 2, 3, 4, 5, 6, 7])
  def test_filter_icing(n):
      """
      test0    : data without icing conditions
      test1    : data 100% filled with icing conditions
      test[2:] : data filled with blocks of icing and non icing conditions.
      """
      global common_path
      path = common_path + "icing/icing_"
      global measure
      measure.import_file_offline(path + "polluted" + str(n) + ".csv")
      meas_df = measure.filter_icing()
      anticipated_df = pd.read_csv(path + "nan" + str(n) + ".csv").drop(
          ["TimeStamp"], axis=1
      )
      assert np.allclose(meas_df.values, anticipated_df.values, equal_nan=True)


  @pytest.mark.parametrize("n", [0, 1])
  def test_tower_shading(n):
      """
      test0 : raw mesoscale data
      test1 : data featuring suitable conditions for tower shading to be computed
      """
      global common_path
      path = common_path + "towershading/towershading_"
      global measure
      measure.import_file_offline(path + "polluted" + str(n) + ".csv")
      meas_df = measure.tower_shading()
      anticipated_df = pd.read_csv(path + "nan" + str(n) + ".csv").drop(
          ["TimeStamp"], axis=1
      )
      assert np.allclose(meas_df.values, anticipated_df.values, equal_nan=True)
#+end_src

***** Steps to run Jupyter Notebook on MS
1.
  - qrsh -q mem.q -l excl=true
  - qrsh
2. module load anaconda/2-5.1.0
3. source activate ~/venv/pse2
4. jupyter notebook --no-browser --port=8875 --ip='*' &
5. on my local machine: msb ac003 8875
**** HeatFlux
#+begin_src python :results file :session
  path = '/ifs/home/aadco/heatflux_svn/pyheatflux/data/iceshapes/simulations/random_iceshape/random_iceshape_9000.dat'

  with open(path, "r", encoding="utf-8-sig") as dat_file:
      dat_lines = [line.strip()
                   for line in dat_file.readlines()
                   if line != '\n']

  shape = np.array(
      [[float(x_coord), float(y_coord)]
       for x_coord, y_coord in
       [re.sub('[0-9]-', ' -', line).split(' ')
        for line in dat_lines[2:-2]]]
  )

  # Flip y coordinates to match experimental data and aerofoil convention
  shape[:, 1] = - shape[:, 1]

  import matplotlib.pyplot as plt
  import matplotlib as mpl
  import matplotlib
  matplotlib.use('Agg')
  # mpl.rcParams['figure.facecolor'] = 'FFFFFF'
  # mpl.rcParams['figure.figsize'] = (15,5)
  fig=plt.figure()
  plt.scatter(shape[:,0],shape[:,1])
  path = 'shape.pdf'
  plt.savefig(path)
  path
#+end_src

**** Topological Data Analysis
- [[file:Library/Geometrical_and_topological_approaches.pdf][Geometrical and topological approaches to Big Data (article)]]
- [[https://www.youtube.com/watch?v=PyKlvRoIYB8][Ann Sizemore, Topological data analysis (youtube)]]
- [[https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction][nonlinear dimensionality reduction (wiki)]]
- [[https://www.youtube.com/watch?v=nq6iPZVUxZU][UMAP (youtube)]]
- [[file:Library/Diffeomorphic-dimensionality-reduction.pdf][Diffeomorphic dimensionality reduction]]

*** Musings                                                       :ARCHIVE:
**** Encode time in a data model
*Recall:* No periodic real mapping is injective.

[[https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/][tutorial]]

**** Centroid of finite set of points (Riemannian manifold)
- [[https://mathoverflow.net/questions/231501/the-mean-of-points-on-a-unit-n-sphere-sn][stackoverflow]]
- [[https://mathoverflow.net/questions/76875/convex-hull-on-a-riemannian-manifold][Convex hull on Riemannian manifolds]]
- [[https://stackoverflow.com/questions/9678624/convex-hull-of-longitude-latitude-points-on-the-surface-of-a-sphere][Geographical ConvexHull]]
- [[https://gis.stackexchange.com/questions/7555/computing-an-averaged-latitude-and-longitude-coordinates][aproaches to site center]]
***** GeoConvexHull
- [[https://github.com/VictorDavis/GeoConvexHull][GeoConvexHull code]]
- [[https://github.com/mgomes/ConvexHull][JS]]
- [[https://github.com/Dhanyatha/GeoSpatial-Operations-using-Apache-Spark][Apache]]
- [[https://github.com/andrewkfiedler/geo-convex-hull][another JS]]
***** [[https://stackoverflow.com/questions/2861272/polygon-area-calculation-using-latitude-and-longitude-generated-from-cartesian-s][Area of polygon, lat lon]]

* Research
** António Machiavelo
<2019-07-30 Tue 16:00-17:00>

* My computing
** Laptop
*Dell XPS 13 9350*
[2016-08-05] 1799€

- Ubuntu 19.04
- git version 2.20.1
- git clone https://github.com/aadcg/emacs.d
- GNU Emacs 26.2 (build 1, x86_64-pc-linux-gnu, GTK+ Version 3.22.30) of 2019-05-23
- aspell pt and ru
- Gnome Tweaks
  - Left CTRL as Compose Key
  - Caps Lock as CTRL
  - Desktop icons
  - Startup (Emacs and Nextcloud Client)
  - Background
  - Fira Code font

- Settings (keyboard shortcuts)
- Firefox
  - Https Everywhere
  - uBlock Origin
  - Dark Reader
  - Saka Keys

- Anaconda (Python 3.7.3)
- GHCI
- Parom TV
- Telegram Desktop
- Thunderbird
- Nextcloud Client

** Desk
[[https://vivo-us.com/collections/monitor-mounts/products/stand-v001o][Monitor arm]]

** Smartphone
Xiaomi Redmi 4X
Lenovo K3
Lenovo A820

Termux
Nextcloud Client

** Niz atom66
[[https://www.nizkeyboard.com/][website]]
[[pdfview:/home/aadco/Downloads/nizatom66_man.pdf::1][manual]]
[[https://www.dropbox.com/sh/ixd9ufpk54m7ve5/AACbyQ3phAN4y3fMQNq0XmuJa?dl=0][firmware&docs]]

*** WAITING Engrave
SCHEDULED: <2019-10-02 Wed>

- State "WAITING"    from              [2019-07-13 Sat 12:17] \\
  Next trip to Russia
[[https://gravernu.ru/lazernaya-gravirovka/texnika/gravirovka-klaviatur-noutbukov?_sm_byp=iVV4s7qqM6RMj1FF][link]]

** Dell UltraSharp U2515h

* Talks
** Emacs
*** Vestas
:PROPERTIES:
:where: Vestas. Porto, Portugal.
:time: 30'
:END:
<2019-05-20 Mon 11:15-12:00>

*** Porto Codes
:PROPERTIES:
:where: Porto i/o. Porto, Portugal.
:time: 30'
:web: [[https://www.meetup.com/portocodes/events/rsjhnqyzmbqb/][meetup]]
:END:
<2019-09-12 Thu 19:00-20:00>

[[file:~/NextCloud/emacs_porto_codes_2019-09-12.flv][video]]

** Happy CommIting!
:PROPERTIES:
:where: Euronext. Porto, Portugal.
:time: 10'
:web: [[https://www.meetup.com/pyporto/events/263218750/][meetup]]
:END:
<2019-08-08 Thu 18:45-20:45>

* TODO Kaggle
DEADLINE: <2019-10-02 Wed>

- State "TODO"       from              [2019-08-21 Wed 23:01]

- [ ] create account
- [ ] in: image, out: pixels
